#!/usr/bin/env python3
"""
ì˜ì–´ ì›Œí¬ë¶ ìë™ ìƒì„± íŒŒì´í”„ë¼ì¸
- ì§€ë¬¸ txt ì…ë ¥ â†’ Claude APIë¡œ ë ˆë²¨ë³„ ì½˜í…ì¸  ìƒì„± â†’ í…œí”Œë¦¿ â†’ PDF
- ë‹¨ê³„ë³„ JSON ì €ì¥ (ì‹¤íŒ¨ ì‹œ í•´ë‹¹ ë‹¨ê³„ë§Œ ì¬ì‹œë„)
- 20ê°œ ì§€ë¬¸ ìˆœì°¨ ì²˜ë¦¬ (ë™ì‹œ í˜¸ì¶œ ì—†ìŒ)
"""
import json, os, sys, time, random, re
from pathlib import Path
from anthropic import Anthropic

# ============================================================
# ì„¤ì •
# ============================================================
API_KEY = os.environ.get("ANTHROPIC_API_KEY", "")
MODEL = "claude-sonnet-4-20250514"
TEMPLATE_DIR = Path(__file__).parent
DATA_DIR = TEMPLATE_DIR / "data"

# ë‚ ì§œ ê¸°ë°˜ ì¶œë ¥ í´ë”: output/2ì›”18ì¼/
from datetime import datetime
TODAY = datetime.now().strftime("%-mì›”%-dì¼") if os.name != 'nt' else datetime.now().strftime("%#mì›”%#dì¼")
OUTPUT_DIR = TEMPLATE_DIR / "output" / TODAY

client = None  # lazy init

def get_client():
    global client
    if client is None:
        if not API_KEY:
            print("âŒ ANTHROPIC_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”")
            sys.exit(1)
        client = Anthropic(api_key=API_KEY)
    return client

# ============================================================
# Claude API í˜¸ì¶œ (ì¬ì‹œë„ í¬í•¨)
# ============================================================
def call_claude(system_prompt: str, user_prompt: str, max_retries=2, max_tokens=4096) -> str:
    """Claude API í˜¸ì¶œ + ì¬ì‹œë„"""
    c = get_client()
    for attempt in range(max_retries + 1):
        try:
            resp = c.messages.create(
                model=MODEL,
                max_tokens=max_tokens,
                temperature=0.3,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}]
            )
            text = resp.content[0].text.strip()
            return text
        except Exception as e:
            print(f"  âš ï¸ ì‹œë„ {attempt+1} ì‹¤íŒ¨: {e}")
            if attempt < max_retries:
                time.sleep(3 * (attempt + 1))
            else:
                raise

def call_claude_json(system_prompt: str, user_prompt: str, max_retries=3, max_tokens=4096) -> dict:
    """Claude API í˜¸ì¶œ â†’ JSON íŒŒì‹± (ê°•í™”ëœ ì¬ì‹œë„ í¬í•¨)"""
    last_error = None
    for attempt in range(max_retries + 1):
        try:
            text = call_claude(system_prompt, user_prompt, max_retries=0, max_tokens=max_tokens)
            return _parse_json_robust(text)
        except (json.JSONDecodeError, ValueError) as e:
            last_error = e
            print(f"  âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{max_retries+1}): {str(e)[:80]}")
            if attempt < max_retries:
                time.sleep(2)
    raise ValueError(f"JSON íŒŒì‹± ìµœì¢… ì‹¤íŒ¨: {last_error}")

def _parse_json_robust(text: str) -> dict:
    """ì—¬ëŸ¬ ì „ëµìœ¼ë¡œ JSON íŒŒì‹± ì‹œë„"""
    # 1) ì½”ë“œë¸”ë¡ ì œê±°
    text = re.sub(r'^```json\s*', '', text.strip())
    text = re.sub(r'\s*```$', '', text.strip())
    text = text.strip()
    
    # 2) ì§ì ‘ íŒŒì‹±
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    
    # 3) JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ê°€ì¥ ë°”ê¹¥ { } ë§¤ì¹­)
    match = re.search(r'\{[\s\S]*\}', text)
    if match:
        try:
            return json.loads(match.group())
        except json.JSONDecodeError:
            pass
    
    # 4) ì´ìŠ¤ì¼€ì´í”„ ì•ˆ ëœ ë”°ì˜´í‘œ ìˆ˜ì •: value ì•ˆì˜ " â†’ \"
    try:
        fixed = _fix_json_quotes(text if not match else match.group())
        return json.loads(fixed)
    except (json.JSONDecodeError, Exception):
        pass
    
    # 5) ì¤„ë°”ê¿ˆ/íƒ­ ì´ìŠ¤ì¼€ì´í”„
    try:
        cleaned = text if not match else match.group()
        # JSON ë¬¸ìì—´ ì•ˆì˜ ì‹¤ì œ ì¤„ë°”ê¿ˆì„ \nìœ¼ë¡œ ë³€í™˜
        cleaned = re.sub(r'(?<=": ")([^"]*?)(?=")', lambda m: m.group(1).replace('\n', '\\n').replace('\t', '\\t'), cleaned)
        return json.loads(cleaned)
    except (json.JSONDecodeError, Exception):
        pass
    
    raise json.JSONDecodeError("ëª¨ë“  íŒŒì‹± ì „ëµ ì‹¤íŒ¨", text[:200], 0)

def _fix_json_quotes(text: str) -> str:
    """JSON ë¬¸ìì—´ ì•ˆì˜ ì´ìŠ¤ì¼€ì´í”„ ì•ˆ ëœ ë”°ì˜´í‘œë¥¼ ìˆ˜ì •"""
    result = []
    in_string = False
    escape_next = False
    for i, ch in enumerate(text):
        if escape_next:
            result.append(ch)
            escape_next = False
            continue
        if ch == '\\':
            result.append(ch)
            escape_next = True
            continue
        if ch == '"':
            if not in_string:
                in_string = True
                result.append(ch)
            else:
                # ë‹¤ìŒ ë¬¸ì í™•ì¸: , } ] : ê³µë°±ì´ë©´ ë¬¸ìì—´ ë
                rest = text[i+1:i+10].lstrip()
                if not rest or rest[0] in ',}]:':
                    in_string = False
                    result.append(ch)
                else:
                    result.append('\\"')  # ì´ìŠ¤ì¼€ì´í”„
                    continue
        else:
            result.append(ch)
    return ''.join(result)

# ============================================================
# ë‹¨ê³„ë³„ ì €ì¥/ë¡œë“œ
# ============================================================
def save_step(passage_dir: Path, step_name: str, data: dict):
    passage_dir.mkdir(parents=True, exist_ok=True)
    path = passage_dir / f"{step_name}.json"
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"  ğŸ’¾ ì €ì¥: {step_name}.json")

def load_step(passage_dir: Path, step_name: str) -> dict | None:
    path = passage_dir / f"{step_name}.json"
    if path.exists():
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return None

# ============================================================
# SYSTEM PROMPT (ê³µí†µ)
# ============================================================
SYS_JSON = """You are an English exam content generator for Korean high school students.
Return ONLY valid JSON. No markdown fences. No explanations. No preamble.
All Korean text must use proper Korean. All English must be grammatically correct."""

SYS_JSON_KR = """ë‹¹ì‹ ì€ í•œêµ­ ê³ ë“±í•™ìƒì„ ìœ„í•œ ì˜ì–´ ì‹œí—˜ ì½˜í…ì¸  ìƒì„±ê¸°ì…ë‹ˆë‹¤.
ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”. ë§ˆí¬ë‹¤ìš´, ì„¤ëª…, ì„œë¬¸ ì—†ì´ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
í•œêµ­ì–´ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ, ì˜ì–´ëŠ” ë¬¸ë²•ì ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."""

# ============================================================
# STEP 1: ê¸°ë³¸ ë¶„ì„ (ì–´íœ˜ + ë²ˆì—­ + í•µì‹¬ë¬¸ì¥)
# ============================================================
def step1_basic_analysis(passage: str, passage_dir: Path) -> dict:
    cached = load_step(passage_dir, "step1_basic")
    if cached:
        print("  âœ… step1 ìºì‹œ ì‚¬ìš©")
        return cached

    sentences_regex = [s.strip() for s in re.split(r'(?<=[.!?])\s+', passage) if s.strip()]
    sent_count = len(sentences_regex)

    print("  ğŸ”„ step1: ê¸°ë³¸ ë¶„ì„ (ì–´íœ˜/ë²ˆì—­/í•µì‹¬ë¬¸ì¥)...")
    prompt = f"""ë‹¤ìŒ ì˜ì–´ ì§€ë¬¸ì„ ë¶„ì„í•˜ì—¬ JSONì„ ìƒì„±í•˜ì„¸ìš”.

[ì§€ë¬¸ - ì´ {sent_count}ê°œ ë¬¸ì¥]
{passage}

[ìƒì„± í•­ëª©]
1. vocab: í•µì‹¬ ì–´íœ˜ 14ê°œ (ê°ê° word, meaning(í•œêµ­ì–´), synonyms(ì˜ì–´ ë™ì˜ì–´ 4ê°œ ì‰¼í‘œêµ¬ë¶„))
2. translation: ì§€ë¬¸ ì „ì²´ì˜ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë²ˆì—­
3. sentences: ì§€ë¬¸ì˜ ëª¨ë“  ë¬¸ì¥ì„ ê°œë³„ ë°°ì—´ë¡œ ë¶„ë¦¬ (ì •í™•íˆ {sent_count}ê°œ!)
   - ì§§ì€ ë¬¸ì¥ë„ ì ˆëŒ€ í•©ì¹˜ì§€ ë§ˆì„¸ìš” (ì˜ˆ: "That's not loyalty." ëŠ” ë…ë¦½ ë¬¸ì¥)
   - ë¬¸ì¥ì„ ì ˆëŒ€ ë¶„ë¦¬í•˜ì§€ ë§ˆì„¸ìš” (ì„¸ë¯¸ì½œë¡  ; ìœ¼ë¡œ ì—°ê²°ëœ ê²ƒì€ 1ë¬¸ì¥)
4. sentence_translations: ê° ë¬¸ì¥ì˜ í•œêµ­ì–´ ë²ˆì—­ (sentencesì™€ ì •í™•íˆ ê°™ì€ ìˆ˜, ê°™ì€ ìˆœì„œ!)
5. key_sentences: ì‹œí—˜ ì¶œì œ ê°€ëŠ¥ì„±ì´ ë†’ì€ í•µì‹¬ ë¬¸ì¥ 8ê°œ (ì›ë¬¸ ê·¸ëŒ€ë¡œ)
6. test_a: vocabì—ì„œ ëœ» ì“°ê¸° í…ŒìŠ¤íŠ¸ìš© 5ê°œ ë‹¨ì–´ (ì˜ì–´)
7. test_b: vocabì—ì„œ ë™ì˜ì–´ í…ŒìŠ¤íŠ¸ìš© 5ê°œ ë‹¨ì–´ (test_aì™€ ê²¹ì¹˜ì§€ ì•Šê²Œ, ì˜ì–´)
8. test_c: vocabì—ì„œ ì² ì í…ŒìŠ¤íŠ¸ìš© 5ê°œ (í•œêµ­ì–´ ëœ»)

JSON í˜•ì‹:
{{
  "vocab": [{{"word":"...", "meaning":"...", "synonyms":"..."}}],
  "translation": "...",
  "sentences": ["...", "..."],
  "sentence_translations": ["ì²«ì§¸ ë¬¸ì¥ í•´ì„...", "ë‘˜ì§¸ ë¬¸ì¥ í•´ì„...", ...],
  "key_sentences": ["...", "..."],
  "test_a": ["...", "..."],
  "test_b": ["...", "..."],
  "test_c": ["...", "..."]
}}"""

    data = call_claude_json(SYS_JSON_KR, prompt, max_tokens=4096)
    
    # ğŸ”’ ê²€ì¦: API ë¬¸ì¥ ë¶„ë¦¬ ëŒ€ì‹  í•­ìƒ regex ì‚¬ìš© (AIê°€ ë¬¸ì¥ì„ í•©ì¹˜ê±°ë‚˜ ìª¼ê°œëŠ” ê²ƒ ë°©ì§€)
    data["sentences"] = sentences_regex
    print(f"  ğŸ“ ë¬¸ì¥ ìˆ˜: {sent_count}ê°œ")
    
    save_step(passage_dir, "step1_basic", data)
    return data

# ============================================================
# ìˆœì„œ ì„ ì§€ ì½”ë“œ ìƒì„± ìœ í‹¸ë¦¬í‹°
# ============================================================
_CIRCLE_NUMS = ["â‘ ","â‘¡","â‘¢","â‘£","â‘¤","â‘¥","â‘¦","â‘§","â‘¨","â‘©"]

def _generate_order_choices(data):
    """
    1) order_paragraphs (A)(B)(C) ë¼ë²¨ì„ ì…”í”Œ â†’ ì •ë‹µì´ í•­ìƒ ABCê°€ ì•„ë‹ˆê²Œ
    2) order_choices 5ì§€ì„ ë‹¤ë¥¼ ì½”ë“œë¡œ ìƒì„±
    3) full_order_blocks ìˆœì„œë„ ì…”í”Œ
    """
    from itertools import permutations
    
    # === 1. 3ë‹¨ë½ ë¼ë²¨ ì…”í”Œ ===
    paras = data.get("order_paragraphs", [])
    if len(paras) == 3:
        # í˜„ì¬: [[A, text1], [B, text2], [C, text3]] (ì›ë¬¸ ìˆœì„œ)
        # ì›ë¬¸ ìˆœì„œ ê¸°ì–µ (ì¸ë±ìŠ¤ 0,1,2 = ì •ë‹µ ìˆœì„œ)
        labels = ["A", "B", "C"]
        random.shuffle(labels)
        # ìƒˆ ë¼ë²¨ ë¶€ì—¬: ì²«ë²ˆì§¸ ë‹¨ë½ â†’ labels[0], ë‘ë²ˆì§¸ â†’ labels[1], ...
        new_paras = [[labels[i], paras[i][1]] for i in range(3)]
        # ì •ë‹µ = labelsë¥¼ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì½ì€ ê²ƒ (labels[0] â†’ labels[1] â†’ labels[2])
        correct = tuple(labels)  # ì˜ˆ: ("C", "A", "B") = ì •ë‹µ
        # í‘œì‹œí•  ë•ŒëŠ” ë¼ë²¨ ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬
        new_paras.sort(key=lambda x: x[0])
        data["order_paragraphs"] = new_paras
    else:
        correct = ("A", "B", "C")
    
    # === 2. ì„ ì§€ 5ê°œ ìƒì„± ===
    all_perms = list(permutations(["A", "B", "C"]))
    wrong = [p for p in all_perms if p != correct]
    selected_wrong = random.sample(wrong, 4)
    all_choices = [correct] + selected_wrong
    random.shuffle(all_choices)
    
    choices = []
    answer = ""
    for i, perm in enumerate(all_choices):
        text = f"({perm[0]})-({perm[1]})-({perm[2]})"
        choices.append(f"{_CIRCLE_NUMS[i]} {text}")
        if perm == correct:
            answer = f"{_CIRCLE_NUMS[i]} {text}"
    data["order_choices"] = choices
    data["order_answer"] = answer
    
    # === 3. ì „ì²´ ë¬¸ì¥ ë°°ì—´ (ì‹¬í™”) ì…”í”Œ ===
    blocks = data.get("full_order_blocks", [])
    if len(blocks) >= 2:
        # ì›ë¬¸ ìˆœì„œ ê¸°ì–µ (ì •ë‹µ)
        original_labels = [b[0] for b in blocks]
        # ìƒˆ ë¼ë²¨ ë¶€ì—¬ + ì…”í”Œ
        n = len(blocks)
        alpha = [chr(65+i) for i in range(n)]  # A, B, C, D, E, ...
        random.shuffle(alpha)
        # ê° ì›ë¬¸ ë¬¸ì¥ì— ìƒˆ ë¼ë²¨
        new_blocks = [[alpha[i], blocks[i][1]] for i in range(n)]
        # ì •ë‹µ ìˆœì„œ = alpha[0] â†’ alpha[1] â†’ ... (ì›ë¬¸ ìˆœì„œëŒ€ë¡œ ë¼ë²¨ ì½ê¸°)
        correct_order = "â†’".join([f"({alpha[i]})" for i in range(n)])
        data["full_order_answer"] = correct_order
        # í‘œì‹œëŠ” ë¼ë²¨ ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬ (ì…”í”Œ íš¨ê³¼!)
        new_blocks.sort(key=lambda x: x[0])
        data["full_order_blocks"] = new_blocks

# ============================================================
# STEP 2: Lv.5 ìˆœì„œ/ì‚½ì…
# ============================================================
def step2_order(passage: str, sentences: list, passage_dir: Path) -> dict:
    cached = load_step(passage_dir, "step2_order")
    if cached:
        print("  âœ… step2 ìºì‹œ ì‚¬ìš©")
        return cached

    print("  ğŸ”„ step2: Lv.5 ìˆœì„œ/ì‚½ì… ìƒì„±...")
    prompt = f"""ë‹¤ìŒ ì˜ì–´ ì§€ë¬¸ìœ¼ë¡œ ìˆœì„œ ë°°ì—´ + ë¬¸ì¥ ì‚½ì… ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

[ì§€ë¬¸]
{passage}

[ê°œë³„ ë¬¸ì¥]
{json.dumps(sentences, ensure_ascii=False)}

[ìƒì„± í•­ëª©]
1. order_intro: ì œì‹œë¬¸ (ì²« 1~2ë¬¸ì¥)
2. order_paragraphs: (A)(B)(C) 3ê°œ ë‹¨ë½ (ê°ê° labelê³¼ text). ì •ë‹µ ìˆœì„œëŠ” ì›ë¬¸ ìˆœì„œëŒ€ë¡œ.
   - ëª¨ë“  ë¬¸ì¥ì´ ë¹ ì§ì—†ì´ í¬í•¨ë˜ì–´ì•¼ í•¨
3. order_choices: 5ì§€ì„ ë‹¤ (í˜•ì‹: "â‘  (A)-(C)-(B)" ë“±). ì •ë‹µ 1ê°œ í¬í•¨.
4. order_answer: ì •ë‹µ ë²ˆí˜¸ (ì˜ˆ: "â‘£ (C)-(A)-(B)")
5. insert_sentence: ì‚½ì…í•  ë¬¸ì¥ 1ê°œ (ì•ë’¤ ë¬¸ë§¥ ë‹¨ì„œê°€ ëª…í™•í•œ ê²ƒ)
6. insert_passage: ì‚½ì… ë¬¸ì¥ì„ ëº€ ë‚˜ë¨¸ì§€ ì§€ë¬¸ì— ( â‘  )~( â‘¤ ) ìœ„ì¹˜ í‘œì‹œ
7. insert_answer: ì‚½ì… ì •ë‹µ ë²ˆí˜¸
8. full_order_blocks: ì „ì²´ ë¬¸ì¥ì„ (A)~ëê¹Œì§€ ê°œë³„ ë¸”ë¡ìœ¼ë¡œ ë¶„í•  (ê°ê° label, text)
9. full_order_answer: ì •ë‹µ ìˆœì„œ (ì˜ˆ: "(C)â†’(G)â†’(D)â†’...")

JSON í˜•ì‹:
{{
  "order_intro": "...",
  "order_paragraphs": [{{"label":"A","text":"..."}}, ...],
  "order_choices": ["â‘  ...", "â‘¡ ...", ...],
  "order_answer": "...",
  "insert_sentence": "...",
  "insert_passage": "...",
  "insert_answer": "...",
  "full_order_blocks": [{{"label":"A","text":"..."}}, ...],
  "full_order_answer": "..."
}}"""

    data = call_claude_json(SYS_JSON, prompt, max_tokens=4096)
    # ë³€í™˜: order_paragraphsë¥¼ [label, text] í˜•íƒœë¡œ
    if data.get("order_paragraphs") and isinstance(data["order_paragraphs"][0], dict):
        data["order_paragraphs"] = [[p["label"], p["text"]] for p in data["order_paragraphs"]]
    if data.get("full_order_blocks") and isinstance(data["full_order_blocks"][0], dict):
        data["full_order_blocks"] = [[b["label"], b["text"]] for b in data["full_order_blocks"]]

    # â˜… ìˆœì„œ ì„ ì§€ë¥¼ ì½”ë“œë¡œ ì§ì ‘ ìƒì„± (AIê°€ ë‹¤ì–‘í•˜ê²Œ ì•ˆ ë§Œë“œëŠ” ë¬¸ì œ í•´ê²°)
    _generate_order_choices(data)

    # ğŸ”’ ê²€ì¦: ì „ì²´ë°°ì—´ ë¸”ë¡ ìˆ˜ vs ì›ë¬¸ ë¬¸ì¥ ìˆ˜
    block_count = len(data.get("full_order_blocks", []))
    sentence_count = len(sentences)
    if block_count != sentence_count:
        print(f"  âš ï¸ ë¬¸ì¥ ìˆ˜ ë¶ˆì¼ì¹˜! ì›ë¬¸ {sentence_count}ê°œ vs ìƒì„± {block_count}ê°œ â†’ ì¬ìƒì„±...")
        # ìºì‹œ ì‚­ì œ í›„ ì¬ì‹œë„ (1íšŒ)
        cache_path = passage_dir / "step2_order.json"
        if cache_path.exists():
            cache_path.unlink()
        data = call_claude_json(SYS_JSON, prompt, max_tokens=4096)
        if data.get("order_paragraphs") and isinstance(data["order_paragraphs"][0], dict):
            data["order_paragraphs"] = [[p["label"], p["text"]] for p in data["order_paragraphs"]]
        if data.get("full_order_blocks") and isinstance(data["full_order_blocks"][0], dict):
            data["full_order_blocks"] = [[b["label"], b["text"]] for b in data["full_order_blocks"]]
        block_count2 = len(data.get("full_order_blocks", []))
        if block_count2 != sentence_count:
            print(f"  âš ï¸ ì¬ì‹œë„ í›„ì—ë„ ë¶ˆì¼ì¹˜ ({block_count2} vs {sentence_count}) â†’ ì›ë¬¸ ë¬¸ì¥ìœ¼ë¡œ ëŒ€ì²´")
            data["full_order_blocks"] = [[chr(65+i), s] for i, s in enumerate(sentences)]

    save_step(passage_dir, "step2_order", data)
    return data

# ============================================================
# STEP 3: Lv.6 ë¹ˆì¹¸ ì¶”ë¡ 
# ============================================================
def step3_blank(passage: str, passage_dir: Path) -> dict:
    cached = load_step(passage_dir, "step3_blank")
    if cached:
        print("  âœ… step3 ìºì‹œ ì‚¬ìš©")
        return cached

    print("  ğŸ”„ step3: Lv.6 ë¹ˆì¹¸ ì¶”ë¡  ìƒì„±...")
    prompt = f"""ë‹¤ìŒ ì˜ì–´ ì§€ë¬¸ìœ¼ë¡œ ë¹ˆì¹¸ ì¶”ë¡  ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

[ì§€ë¬¸]
{passage}

[ê·œì¹™]
- ì£¼ì œë¬¸(ê²°ë¡ ë¬¸)ì˜ í•µì‹¬ ë¶€ë¶„ì„ ë¹ˆì¹¸ìœ¼ë¡œ ë§Œë“¤ê¸°
- ë¹ˆì¹¸ì€ 15ë‹¨ì–´ ì´ë‚´ë¡œ (ë„ˆë¬´ ê¸´ ë¹ˆì¹¸ ê¸ˆì§€)
- ë¹ˆì¹¸ ë¬¸ì¥ ì™¸ì˜ ë‹¤ë¥¸ ë¬¸ì¥ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€ (ìƒëµ/ì¶•ì•½/ë³€í˜• ì ˆëŒ€ ê¸ˆì§€)
- ë¹ˆì¹¸ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë¬¸ì¥ ë¶€ë¶„ë„ ì ˆëŒ€ ë³€í˜•í•˜ì§€ ë§ ê²ƒ
- ì„ ì§€ 12ê°œ: ì •ë‹µ 6~7ê°œ + ì˜¤ë‹µ 5~6ê°œ
- ì •ë‹µ: ì›ë¬¸ í•µì‹¬ í‘œí˜„ì„ ë™ì˜ì–´/ë¹„ìœ ì  í‘œí˜„ìœ¼ë¡œ ë³€í˜•
- ì˜¤ë‹µ: ì§€ë¬¸ ë‚´ìš© ì™œê³¡, ë°˜ëŒ€ ì˜ë¯¸, ë¯¸ì–¸ê¸‰ ë‚´ìš©
- ê° ì„ ì§€ëŠ” 15ë‹¨ì–´ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ

[JSON í˜•ì‹]
{{
  "blank_passage": "ë¹ˆì¹¸ì´ í¬í•¨ëœ ì „ì²´ ì§€ë¬¸ (ë¹ˆì¹¸ì€ ____ë¡œ í‘œì‹œ)",
  "blank_answer_korean": "ë¹ˆì¹¸ ì •ë‹µ ë‚´ìš© í•œêµ­ì–´",
  "blank_options": ["â‘  ...", "â‘¡ ...", ... "â‘« ..."],
  "blank_correct": ["â‘¡", "â‘¢", "â‘¤", ...],
  "blank_wrong": ["â‘ ", "â‘£", ...]
}}"""

    data = call_claude_json(SYS_JSON, prompt, max_tokens=3000)
    save_step(passage_dir, "step3_blank", data)
    return data

# ============================================================
# STEP 4: Lv.7 ì£¼ì œ ì°¾ê¸°
# ============================================================
def step4_topic(passage: str, passage_dir: Path) -> dict:
    cached = load_step(passage_dir, "step4_topic")
    if cached:
        print("  âœ… step4 ìºì‹œ ì‚¬ìš©")
        return cached

    print("  ğŸ”„ step4: Lv.7 ì£¼ì œ ì°¾ê¸° ìƒì„±...")
    prompt = f"""ë‹¤ìŒ ì˜ì–´ ì§€ë¬¸ìœ¼ë¡œ ì£¼ì œ ì°¾ê¸° ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

[ì§€ë¬¸]
{passage}

[ê·œì¹™]
- ì§€ë¬¸ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ (ìƒëµ/ë³€í˜• ê¸ˆì§€)
- ì„ ì§€ 12ê°œ: ì •ë‹µ 5ê°œ + ì˜¤ë‹µ 7ê°œ
- ì„ ì§€ëŠ” ë°˜ë“œì‹œ ì˜ì–´ë¡œ ì‘ì„± (í•œêµ­ì–´ ê¸ˆì§€)
- ì •ë‹µ: ì£¼ì œë¬¸ í‚¤ì›Œë“œë¥¼ ë™ì˜ì–´ë¡œ ì¹˜í™˜í•œ ì˜ì–´ í‘œí˜„
- ì˜¤ë‹µ: ì§€ë¬¸ ë¯¸ì–¸ê¸‰, ë¶€ë¶„ì  ë‚´ìš©, ì™œê³¡ (ì˜ì–´)
- ì¶”ë¡ ì  ì‚¬ê³  ê¸ˆì§€: ê¸€ì—ì„œ ì§ì ‘ ì–¸ê¸‰ëœ ë‚´ìš©ë§Œ ì •ë‹µ
- ê° ì„ ì§€ëŠ” 30ë‹¨ì–´ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ

[JSON í˜•ì‹]
{{
  "topic_passage": "ì›ë¬¸ ì „ë¬¸ (ê·¸ëŒ€ë¡œ)",
  "topic_options": ["â‘  the importance of...", "â‘¡ how to...", ... "â‘« ..."],
  "topic_correct": ["â‘¡", "â‘£", ...],
  "topic_wrong": ["â‘ ", "â‘¢", ...]
}}"""

    data = call_claude_json(SYS_JSON, prompt, max_tokens=3000)
    save_step(passage_dir, "step4_topic", data)
    return data

# ============================================================
# STEP 5: Lv.8 ì–´ë²•
# ============================================================
def step5_grammar(passage: str, passage_dir: Path) -> dict:
    cached = load_step(passage_dir, "step5_grammar")
    if cached:
        print("  âœ… step5 ìºì‹œ ì‚¬ìš©")
        return cached

    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', passage) if s.strip()]
    sent_count = len(sentences)
    error_count = min(8, sent_count)  # ë¬¸ì¥ ìˆ˜ë³´ë‹¤ ë§ì€ ì˜¤ë¥˜ ë¶ˆê°€
    bracket_count = min(13, sent_count * 2)  # ë¬¸ì¥ë‹¹ ìµœëŒ€ 2ê°œ ê´„í˜¸
    
    print("  ğŸ”„ step5: Lv.8 ì–´ë²• ìƒì„±...")
    prompt = f"""ë‹¤ìŒ ì˜ì–´ ì§€ë¬¸ìœ¼ë¡œ ì–´ë²• ë¬¸ì œ 2ì¢…ë¥˜ë¥¼ ìƒì„±í•˜ì„¸ìš”.

[ì›ë¬¸ - ì´ {sent_count}ê°œ ë¬¸ì¥]
{passage}

[âš ï¸ ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™]
1. ì›ë¬¸ì€ ì •í™•íˆ {sent_count}ê°œ ë¬¸ì¥ì…ë‹ˆë‹¤
2. ì¶œë ¥ ì§€ë¬¸ë„ ë°˜ë“œì‹œ ì •í™•íˆ {sent_count}ê°œ ë¬¸ì¥ì´ì–´ì•¼ í•©ë‹ˆë‹¤
3. ì ˆëŒ€ ë¬¸ì¥ì„ ì¶”ê°€/ì‚­ì œ/ë¶„ë¦¬/í•©ì¹˜ê¸° í•˜ì§€ ë§ˆì„¸ìš”
4. ì›ë¬¸ ë¬¸ì¥ì— ê´„í˜¸ë‚˜ ì˜¤ë¥˜ë§Œ ì‚½ì…í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€
5. ë¬¸ì¥ ìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ ì˜¤ë¥˜/ê´„í˜¸ ìˆ˜ë¥¼ ì¤„ì´ì„¸ìš” (ë¬¸ì¥ ì¶”ê°€ëŠ” ì ˆëŒ€ ê¸ˆì§€!)

[ì–´ë²• ê´„í˜¸í˜• Lv.8-1]
- ì›ë¬¸ {sent_count}ê°œ ë¬¸ì¥ ëª¨ë‘ í¬í•¨ (ì¶œì œ ì•ˆ í•˜ëŠ” ë¬¸ì¥ë„ ì›ë¬¸ ê·¸ëŒ€ë¡œ)
- {bracket_count}ê°œ ê´„í˜¸: (N)[ì •ë‹µ / ì˜¤ë‹µ] í˜•íƒœ
- í•œ ë¬¸ì¥ì— ì—¬ëŸ¬ ê´„í˜¸ ê°€ëŠ¥
- ì¶œì œ: ì‹œì œ, ëŒ€ëª…ì‚¬, ë™ëª…ì‚¬, toë¶€ì •ì‚¬, í˜•ìš©ì‚¬/ë¶€ì‚¬, ê´€ê³„ëŒ€ëª…ì‚¬, ë¶„ì‚¬, ì‚¬ì—­ë™ì‚¬ ë“±

[ì–´ë²• ì„œìˆ í˜• Lv.8-2]
- ì›ë¬¸ {sent_count}ê°œ ë¬¸ì¥ ëª¨ë‘ í¬í•¨
- {error_count}ê°œ ë¬¸ë²• ì˜¤ë¥˜ ì‚½ì… (ë°‘ì¤„ ì—†ì´)
- í•œ ë¬¸ì¥ì— ìµœëŒ€ 1ê°œ ì˜¤ë¥˜
- ë¬¸ì¥ì´ {sent_count}ê°œë¿ì´ë¯€ë¡œ ì˜¤ë¥˜ë„ ìµœëŒ€ {error_count}ê°œë§Œ!

[JSON í˜•ì‹]
{{
  "grammar_bracket_passage": "ê´„í˜¸ í¬í•¨ ì „ì²´ ì§€ë¬¸ (ì •í™•íˆ {sent_count}ë¬¸ì¥)",
  "grammar_bracket_count": {bracket_count},
  "grammar_bracket_answers": [{{"num":1, "answer":"go", "wrong":"will go", "reason":"if ì¡°ê±´ì ˆ í˜„ì¬ì‹œì œ"}}, ...],
  "grammar_error_passage": "ì˜¤ë¥˜ í¬í•¨ ì „ì²´ ì§€ë¬¸ (ì •í™•íˆ {sent_count}ë¬¸ì¥)",
  "grammar_error_count": {error_count},
  "grammar_error_answers": [{{"num":1, "original":"watch", "error":"watching", "reason":"tend to + ë™ì‚¬ì›í˜•"}}, ...]
}}"""

    data = call_claude_json(SYS_JSON, prompt, max_tokens=4000)
    
    # ğŸ”’ ê²€ì¦: ë¬¸ì¥ ìˆ˜ ì²´í¬
    for key in ['grammar_bracket_passage', 'grammar_error_passage']:
        gen_text = data.get(key, '')
        gen_sents = len([s for s in re.split(r'(?<=[.!?])\s+', gen_text) if s.strip()])
        if gen_sents > sent_count + 1:
            print(f"  âš ï¸ {key}: {gen_sents}ë¬¸ì¥ (ì›ë¬¸ {sent_count}) â†’ ì¬ìƒì„±...")
            cache_path = passage_dir / "step5_grammar.json"
            if cache_path.exists():
                cache_path.unlink()
            data = call_claude_json(SYS_JSON, prompt, max_tokens=4000)
            break
    
    save_step(passage_dir, "step5_grammar", data)
    return data

# ============================================================
# STEP 6: Lv.9 ì–´íœ˜ì‹¬í™” + ë‚´ìš©ì¼ì¹˜
# ============================================================
def step6_vocab_content(passage: str, passage_dir: Path) -> dict:
    cached = load_step(passage_dir, "step6_vocab_content")
    if cached:
        print("  âœ… step6 ìºì‹œ ì‚¬ìš©")
        return cached

    print("  ğŸ”„ step6: Lv.9 ì–´íœ˜ì‹¬í™” + ë‚´ìš©ì¼ì¹˜ ìƒì„±...")
    prompt = f"""ë‹¤ìŒ ì˜ì–´ ì§€ë¬¸ìœ¼ë¡œ ì–´íœ˜ ì‹¬í™” + ë‚´ìš© ì¼ì¹˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

[ì§€ë¬¸]
{passage}

[Lv.9-1 Part A ê·œì¹™]
- ì›ë¬¸ì˜ ëª¨ë“  ë¬¸ì¥ì„ ë¹ ì§ì—†ì´ í¬í•¨
- 9ê°œ ê´„í˜¸: (N)[ì •ë‹µ / í˜¼ë™ì–´] (ì² ì/ë°œìŒ ìœ ì‚¬í•œ í˜¼ë™ì–´)

[Lv.9-1 Part B ê·œì¹™]
- 10ê°œ ë‹¨ì–´ (ìµœì†Œ 8ê°œ, ê°€ëŠ¥í•˜ë©´ 10ê°œ), ê° 5ê°œ ì„ íƒì§€ (ì •ë‹µ 2~3ê°œ)
- ì˜¤ë‹µ: ì² ì/ë°œìŒ ìœ ì‚¬í•˜ì§€ë§Œ ì˜ë¯¸ ë‹¤ë¥¸ ë‹¨ì–´
- ì¤‘ìš”: ê° ë‹¨ì–´ì˜ 5ê°œ ì„ íƒì§€ì— ì¤‘ë³µ ë‹¨ì–´ê°€ ì ˆëŒ€ ì—†ì–´ì•¼ í•¨ (ì˜ˆ: maximize/maximize ê¸ˆì§€)
- ì¤‘ìš”: í•´ë‹¹ ë‹¨ì–´ ìì²´ë¥¼ ì„ ì§€ì— í¬í•¨ ê¸ˆì§€ (ì˜ˆ: regardì˜ ì„ ì§€ì— regard ë„£ì§€ ë§ ê²ƒ)

[Lv.9-2 ê·œì¹™]
- í•œêµ­ì–´ ì„ ì§€ 10ê°œ (ì •ë‹µ 6ê°œ, ì˜¤ë‹µ 4ê°œ - í‚¤ì›Œë“œ 1ê°œ ë³€í˜•)
- ì˜ì–´ ì„ ì§€ 10ê°œ (ì •ë‹µ 5ê°œ, ì˜¤ë‹µ 5ê°œ - í‚¤ì›Œë“œ 1ê°œ ë³€í˜•)

[JSON í˜•ì‹]
{{
  "vocab_advanced_passage": "ê´„í˜¸ í¬í•¨ ì§€ë¬¸",
  "vocab_parta_answers": [{{"num":1, "answer":"tend", "wrong":"intend"}}, ...],
  "vocab_partb": [{{"word":"tend to", "choices":"be inclined to / intend to / be prone to / pretend to / be apt to"}}, ...],
  "vocab_partb_answers": [{{"num":1, "correct":["be inclined to", "be prone to", "be apt to"]}}, ...],
  "content_match_kr": ["â‘  í‰ì†Œ ê·¹ì¥ì—ì„œ í˜¼ì ì˜í™”ë¥¼ ë³¸ë‹¤.", ...],
  "content_match_kr_answer": ["â‘¡", "â‘¢", ...],
  "content_match_en": ["â‘  The writer normally watches movies alone.", ...],
  "content_match_en_answer": ["â‘¡", "â‘£", ...]
}}"""

    data = call_claude_json(SYS_JSON_KR, prompt, max_tokens=4000)
    save_step(passage_dir, "step6_vocab_content", data)
    return data

# ============================================================
# STEP 7: Lv.10 ì˜ì‘ (API ë¶ˆí•„ìš” - í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ ì²˜ë¦¬)
# ============================================================
def step7_writing(sentences: list, translation: str, passage_dir: Path, sentence_translations: list = None) -> dict:
    cached = load_step(passage_dir, "step7_writing")
    if cached:
        print("  âœ… step7 ìºì‹œ ì‚¬ìš©")
        return cached

    print("  ğŸ”„ step7: Lv.10 ì˜ì‘ ìƒì„± (ë¡œì»¬ ì²˜ë¦¬)...")
    # í•œêµ­ì–´ ë¬¸ì¥: sentence_translations ìš°ì„ , ì—†ìœ¼ë©´ translation ë¶„ë¦¬
    if sentence_translations and len(sentence_translations) >= len(sentences):
        kr_sentences = sentence_translations
    else:
        kr_sentences = [s.strip() for s in re.split(r'(?<=[.!?ë‹¤ìš”ìŒì„])\s+', translation) if s.strip()]

    writing_items = []
    for i, eng in enumerate(sentences):
        words = eng.split()
        # ëŒ€ë¬¸ìâ†’ì†Œë¬¸ì ë³€í™˜ (ì²« ë‹¨ì–´, I/ê³ ìœ ëª…ì‚¬ ì œì™¸)
        processed = []
        for j, w in enumerate(words):
            if j == 0 and w[0].isupper() and w not in ['I', 'I,']:
                # ê³ ìœ ëª…ì‚¬ ì²´í¬ (ê°„ë‹¨íˆ: 2ê¸€ì ì´ìƒ ëŒ€ë¬¸ì ì‹œì‘)
                if not (len(w) > 1 and w[1:].islower() and w[0].isupper() and any(c.isupper() for c in w)):
                    w = w[0].lower() + w[1:]
            processed.append(w)
        # ë§ˆì¹¨í‘œ/ëŠë‚Œí‘œ/ë¬¼ìŒí‘œ ì œê±°
        last = processed[-1]
        if last.endswith(('.', '!', '?')):
            processed[-1] = last[:-1]
        # ì…”í”Œ
        shuffled = processed.copy()
        random.shuffle(shuffled)
        scrambled = ' / '.join(shuffled)

        kr = kr_sentences[i] if i < len(kr_sentences) else f"ë¬¸ì¥ {i+1}"
        writing_items.append({
            "korean": kr,
            "scrambled": scrambled,
            "answer": eng
        })

    data = {"writing_items": writing_items}
    save_step(passage_dir, "step7_writing", data)
    return data

# ============================================================
# STEP 8: ì •ë‹µ ìƒì„±
# ============================================================
def step8_answers(all_data: dict, passage_dir: Path) -> dict:
    cached = load_step(passage_dir, "step8_answers")
    if cached:
        print("  âœ… step8 ìºì‹œ ì‚¬ìš©")
        return cached

    print("  ğŸ”„ step8: ì •ë‹µ í˜ì´ì§€ ìƒì„±...")
    # ì •ë‹µ HTML ìƒì„±
    lines = []

    # Lv.1
    lines.append('<p class="ast">Lv.1 ì–´íœ˜ í…ŒìŠ¤íŠ¸</p>')
    lines.append('<p>A. (ì–´íœ˜ í…ŒìŠ¤íŠ¸ ì •ë‹µì€ í•™ìƒì´ ì§ì ‘ í™•ì¸)</p>')

    # Lv.5
    s2 = all_data.get("step2", {})
    lines.append('<p class="ast">Lv.5 ìˆœì„œ ë°°ì—´</p>')
    lines.append(f'<p>ì •ë‹µ: {s2.get("order_answer","")}</p>')
    lines.append(f'<p>ì‚½ì… ì •ë‹µ: {s2.get("insert_answer","")}</p>')
    lines.append(f'<p>ì „ì²´ ë°°ì—´: {s2.get("full_order_answer","")}</p>')

    # Lv.6
    s3 = all_data.get("step3", {})
    correct = ', '.join(s3.get("blank_correct", []))
    lines.append('<p class="ast">Lv.6 ë¹ˆì¹¸ ì¶”ë¡ </p>')
    lines.append(f'<p>ì •ë‹µ: {correct}</p>')

    # Lv.7
    s4 = all_data.get("step4", {})
    correct = ', '.join(s4.get("topic_correct", []))
    lines.append('<p class="ast">Lv.7 ì£¼ì œ ì°¾ê¸°</p>')
    lines.append(f'<p>ì •ë‹µ: {correct}</p>')

    # Lv.8 ê´„í˜¸
    s5 = all_data.get("step5", {})
    lines.append('<p class="ast">Lv.8 ì–´ë²• (ê´„í˜¸)</p>')
    for a in s5.get("grammar_bracket_answers", []):
        if isinstance(a, dict):
            lines.append(f'<p>({a.get("num","")}) {a.get("answer","")}</p>')

    # Lv.8 ì„œìˆ í˜•
    lines.append('<p class="ast">Lv.8 ì„œìˆ í˜•</p>')
    for a in s5.get("grammar_error_answers", []):
        if isinstance(a, dict):
            lines.append(f'<p>{a.get("error","")}->{a.get("original","")}({a.get("reason","")})</p>')

    # Lv.9-1
    s6 = all_data.get("step6", {})
    lines.append('<p class="ast">Lv.9-1 ì–´íœ˜ Part A</p>')
    for a in s6.get("vocab_parta_answers", []):
        if isinstance(a, dict):
            lines.append(f'<p>({a.get("num","")}) {a.get("answer","")}</p>')

    lines.append('<p class="ast">Lv.9-1 ì–´íœ˜ Part B</p>')
    for a in s6.get("vocab_partb_answers", []):
        if isinstance(a, dict):
            correct_list = ', '.join(a.get("correct", []))
            lines.append(f'<p>{a.get("num","")}: {correct_list}</p>')

    # Lv.9-2
    kr_ans = ', '.join(s6.get("content_match_kr_answer", []))
    en_ans = ', '.join(s6.get("content_match_en_answer", []))
    lines.append('<p class="ast">Lv.9-2 ë‚´ìš©ì¼ì¹˜</p>')
    lines.append(f'<p>í•œêµ­ì–´: {kr_ans}</p>')
    lines.append(f'<p>ì˜ì–´: {en_ans}</p>')

    # Lv.10
    s7 = all_data.get("step7", {})
    lines.append('<p class="ast">Lv.10 ì˜ì‘</p>')
    for item in s7.get("writing_items", []):
        lines.append(f'<p>{item.get("answer","")}</p>')

    answers_html = '\n'.join(lines)
    data = {"answers_html": answers_html}
    save_step(passage_dir, "step8_answers", data)
    return data

# ============================================================
# ì „ì²´ ë°ì´í„° â†’ í…œí”Œë¦¿ ë³€ìˆ˜ë¡œ ë³€í™˜
# ============================================================
def merge_to_template_data(passage: str, meta: dict, all_steps: dict) -> dict:
    """ëª¨ë“  ë‹¨ê³„ ê²°ê³¼ë¥¼ í…œí”Œë¦¿ ë³€ìˆ˜ë¡œ ë³‘í•©"""
    s1 = all_steps["step1"]
    s2 = all_steps["step2"]
    s3 = all_steps["step3"]
    s4 = all_steps["step4"]
    s5 = all_steps["step5"]
    s6 = all_steps["step6"]
    s7 = all_steps["step7"]
    s8 = all_steps["step8"]

    return {
        # ë©”íƒ€ ì •ë³´
        "subject": meta.get("subject", ""),
        "publisher": meta.get("publisher", ""),
        "lesson_num": meta.get("lesson_num", ""),
        "lesson_n": meta.get("lesson_n", ""),
        "challenge_title": meta.get("challenge_title", ""),
        # ì§€ë¬¸/ë²ˆì—­
        "passage": passage,
        "translation": s1.get("translation", ""),
        # Lv.1 ì–´íœ˜
        "vocab": s1.get("vocab", []),
        "test_a": s1.get("test_a", []),
        "test_b": s1.get("test_b", []),
        "test_c": s1.get("test_c", []),
        # Lv.3 ë¬¸ì¥ë¶„ì„ (ì „ì²´ ë¬¸ì¥) + í•µì‹¬ë¬¸ì¥
        "sentences": s1.get("sentences", []),
        "key_sentences": s1.get("key_sentences", []),
        # Lv.5 ìˆœì„œ/ì‚½ì…
        "order_intro": s2.get("order_intro", ""),
        "order_paragraphs": s2.get("order_paragraphs", []),
        "order_choices": s2.get("order_choices", []),
        "insert_sentence": s2.get("insert_sentence", ""),
        "insert_passage": s2.get("insert_passage", ""),
        "full_order_blocks": s2.get("full_order_blocks", []),
        # Lv.6 ë¹ˆì¹¸
        "blank_passage": s3.get("blank_passage", ""),
        "blank_options": s3.get("blank_options", []),
        # Lv.7 ì£¼ì œ
        "topic_passage": s4.get("topic_passage", ""),
        "topic_options": s4.get("topic_options", []),
        # Lv.8 ì–´ë²•
        "grammar_bracket_passage": s5.get("grammar_bracket_passage", ""),
        "grammar_bracket_count": s5.get("grammar_bracket_count", 13),
        "grammar_error_passage": s5.get("grammar_error_passage", ""),
        "grammar_error_count": s5.get("grammar_error_count", 8),
        # Lv.9
        "vocab_advanced_passage": s6.get("vocab_advanced_passage", ""),
        "vocab_partb": s6.get("vocab_partb", []),
        "content_match_kr": s6.get("content_match_kr", []),
        "content_match_en": s6.get("content_match_en", []),
        # Lv.10 ì˜ì‘
        "writing_items": s7.get("writing_items", []),
        # ì •ë‹µ
        "answers_html": s8.get("answers_html", ""),
    }

# ============================================================
# PDF ë Œë”ë§
# ============================================================
def _unique_path(directory: Path, base_name: str, ext: str) -> Path:
    """ê°™ì€ ì´ë¦„ íŒŒì¼ì´ ìˆìœ¼ë©´ _v2, _v3 ë“± ë¶™ì—¬ì„œ ê³ ìœ  ê²½ë¡œ ë°˜í™˜"""
    path = directory / f"{base_name}{ext}"
    if not path.exists() and not path.with_suffix('.html').exists():
        return path
    v = 2
    while True:
        path = directory / f"{base_name}_v{v}{ext}"
        if not path.exists() and not path.with_suffix('.html').exists():
            return path
        v += 1

def render_pdf(template_data: dict, output_path: Path, levels=None):
    """Jinja2 â†’ HTML ì €ì¥ (í¬ë¡¬ì—ì„œ PDF ì¸ì‡„)"""
    from jinja2 import Environment, FileSystemLoader

    env = Environment(loader=FileSystemLoader(str(TEMPLATE_DIR)))
    tmpl = env.get_template("template.html")
    template_data["levels"] = levels  # Noneì´ë©´ ì „ì²´ ì¶œë ¥
    html = tmpl.render(**template_data)

    # WeasyPrint ì‹œë„, ì—†ìœ¼ë©´ HTMLë¡œ ì €ì¥
    try:
        from weasyprint import HTML
        HTML(string=html).write_pdf(str(output_path))
        print(f"  ğŸ“„ PDF ìƒì„±: {output_path.name}")
    except (ImportError, OSError):
        html_path = output_path.with_suffix('.html')
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html)
        print(f"  ğŸ“„ HTML ìƒì„±: {html_path.name}")
        print(f"  â„¹ï¸  í¬ë¡¬ì—ì„œ ì—´ê³  Ctrl+P â†’ PDFë¡œ ì €ì¥í•˜ì„¸ìš”")

# ============================================================
# ë©”ì¸: ë‹¨ì¼ ì§€ë¬¸ ì²˜ë¦¬
# ============================================================
def process_passage(passage: str, meta: dict, passage_id: str, force=False, levels=None):
    """ì§€ë¬¸ 1ê°œ â†’ ì „ì²´ ì›Œí¬ë¶ ìƒì„±"""
    passage_dir = DATA_DIR / passage_id
    if force:
        import shutil
        if passage_dir.exists():
            shutil.rmtree(passage_dir)

    print(f"\n{'='*50}")
    print(f"ğŸ“ ì§€ë¬¸ ì²˜ë¦¬: {passage_id} ({meta.get('challenge_title','')})")
    print(f"{'='*50}")

    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', passage) if s.strip()]
    all_steps = {}

    # Step 1: ê¸°ë³¸ ë¶„ì„
    all_steps["step1"] = step1_basic_analysis(passage, passage_dir)
    sentences_from_api = all_steps["step1"].get("sentences", sentences)

    # Step 2: Lv.5 ìˆœì„œ/ì‚½ì…
    all_steps["step2"] = step2_order(passage, sentences_from_api, passage_dir)

    # Step 3: Lv.6 ë¹ˆì¹¸
    all_steps["step3"] = step3_blank(passage, passage_dir)

    # Step 4: Lv.7 ì£¼ì œ
    all_steps["step4"] = step4_topic(passage, passage_dir)

    # Step 5: Lv.8 ì–´ë²•
    all_steps["step5"] = step5_grammar(passage, passage_dir)

    # Step 6: Lv.9 ì–´íœ˜+ë‚´ìš©ì¼ì¹˜
    all_steps["step6"] = step6_vocab_content(passage, passage_dir)

    # Step 7: Lv.10 ì˜ì‘ (ë¡œì»¬)
    translation = all_steps["step1"].get("translation", "")
    sentence_translations = all_steps["step1"].get("sentence_translations", [])
    all_steps["step7"] = step7_writing(sentences_from_api, translation, passage_dir, sentence_translations)

    # Step 8: ì •ë‹µ
    all_steps["step8"] = step8_answers(all_steps, passage_dir)

    # ë³‘í•© + PDF
    template_data = merge_to_template_data(passage, meta, all_steps)

    # ğŸ”’ ì½˜í…ì¸  ê¸¸ì´ ê²€ì¦ (í˜ì´ì§€ ë°€ë¦¼ ë°©ì§€)
    warnings = []
    bp = template_data.get("blank_passage", "")
    if len(bp) > 1200:
        warnings.append(f"blank_passage ê¸¸ì´ {len(bp)} (ê¶Œì¥ 1200 ì´ë‚´)")
    gp = template_data.get("grammar_bracket_passage", "")
    if len(gp) > 1600:
        warnings.append(f"grammar_bracket_passage ê¸¸ì´ {len(gp)} (ê¶Œì¥ 1600 ì´ë‚´)")
    gep = template_data.get("grammar_error_passage", "")
    if len(gep) > 1200:
        warnings.append(f"grammar_error_passage ê¸¸ì´ {len(gep)} (ê¶Œì¥ 1200 ì´ë‚´)")
    if warnings:
        print(f"  âš ï¸ ì½˜í…ì¸  ê¸¸ì´ ê²½ê³  (í˜ì´ì§€ ë°€ë¦¼ ê°€ëŠ¥):")
        for w in warnings:
            print(f"     - {w}")

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    base_name = f"{meta.get('lesson_num','')}ê³¼_{meta.get('challenge_title','ì›Œí¬ë¶')}_ì›Œí¬ë¶"
    pdf_path = _unique_path(OUTPUT_DIR, base_name, ".pdf")
    render_pdf(template_data, pdf_path, levels=levels)

    print(f"âœ… ì™„ë£Œ: {pdf_path.name}")
    return pdf_path

# ============================================================
# ë°°ì¹˜ ì²˜ë¦¬: ì—¬ëŸ¬ ì§€ë¬¸
# ============================================================
def process_batch(passages: list[dict], levels=None):
    """ì—¬ëŸ¬ ì§€ë¬¸ì„ ìˆœì°¨ ì²˜ë¦¬
    passages: [{"passage": "...", "meta": {...}, "id": "01"}, ...]
    """
    results = []
    total = len(passages)
    for i, item in enumerate(passages):
        print(f"\nğŸ”µ [{i+1}/{total}] ì²˜ë¦¬ ì‹œì‘...")
        try:
            pdf = process_passage(item["passage"], item["meta"], item["id"], levels=levels)
            results.append({"id": item["id"], "status": "done", "pdf": str(pdf)})
        except Exception as e:
            print(f"âŒ ì‹¤íŒ¨: {item['id']} - {e}")
            results.append({"id": item["id"], "status": "error", "error": str(e)})

    # ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*50}")
    print("ğŸ“Š ê²°ê³¼ ìš”ì•½")
    done = sum(1 for r in results if r["status"] == "done")
    err = sum(1 for r in results if r["status"] == "error")
    print(f"  âœ… ì„±ê³µ: {done}/{total}")
    if err:
        print(f"  âŒ ì‹¤íŒ¨: {err}/{total}")
        for r in results:
            if r["status"] == "error":
                print(f"     - {r['id']}: {r['error']}")
    return results


# ============================================================
# ë‹¨ì¼ íŒŒì¼ì—ì„œ ì—¬ëŸ¬ ì§€ë¬¸ ìë™ ë¶„ë¦¬ + ì‹¤í–‰
# ============================================================
def split_and_run(filepath: str, lesson_num: str = "5", levels=None):
    """
    ###ì œëª©### êµ¬ë¶„ìë¡œ ë‚˜ë‰œ ë‹¨ì¼ íŒŒì¼ì—ì„œ ì§€ë¬¸ ì¶”ì¶œ â†’ ìˆœì°¨ ì‹¤í–‰
    
    íŒŒì¼ í˜•ì‹:
        ###05ê°• 01ë²ˆ###
        ì§€ë¬¸ ì˜ì–´ í…ìŠ¤íŠ¸...
        
        ###05ê°• 02ë²ˆ###
        ì§€ë¬¸ ì˜ì–´ í…ìŠ¤íŠ¸...
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ###...### íŒ¨í„´ìœ¼ë¡œ ë¶„ë¦¬
    parts = re.split(r'###(.+?)###', content)
    # parts = ['', '05ê°• 01ë²ˆ', 'ì§€ë¬¸ë‚´ìš©', '05ê°• 02ë²ˆ', 'ì§€ë¬¸ë‚´ìš©', ...]
    
    passages = []
    for i in range(1, len(parts), 2):
        title = parts[i].strip()
        text = parts[i+1].strip() if i+1 < len(parts) else ""
        if text:
            # ì œëª©ì—ì„œ ê³¼ë²ˆí˜¸ + ë²ˆí˜¸ ì¶”ì¶œ
            lesson_match = re.search(r'(\d+)ê°•', title)
            num_match = re.search(r'(\d+)ë²ˆ', title)
            lnum = lesson_match.group(1) if lesson_match else lesson_num
            pid = num_match.group(1).zfill(2) if num_match else str(len(passages)+1).zfill(2)
            passages.append({
                "id": f"{lnum}_{pid}",
                "passage": text,
                "meta": {
                    "subject": "ìˆ˜íŠ¹ ì˜ì–´", "publisher": "EBS",
                    "lesson_num": lnum, "lesson_n": lnum,
                    "challenge_title": title
                }
            })
    
    if not passages:
        print("âŒ ì§€ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ###ì œëª©### í˜•ì‹ìœ¼ë¡œ êµ¬ë¶„í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    print(f"ğŸ“š ì´ {len(passages)}ê°œ ì§€ë¬¸ ë°œê²¬!")
    for p in passages:
        print(f"  - {p['meta']['challenge_title']}")
    print()
    
    process_batch(passages, levels=levels)

    # ìë™ìœ¼ë¡œ HTML í•©ì¹˜ê¸°
    merge_html_files()


# ============================================================
# HTML í•©ì¹˜ê¸° (ì—¬ëŸ¬ ì›Œí¬ë¶ â†’ í•˜ë‚˜ì˜ HTML)
# ============================================================
def merge_html_files(output_dir=None):
    """output í´ë”ì˜ ëª¨ë“  HTMLì„ í•˜ë‚˜ë¡œ í•©ì¹¨ (í•©ë³¸ íŒŒì¼ëª… ìë™ ìƒì„±)"""
    if output_dir is None:
        output_dir = OUTPUT_DIR
    
    html_files = sorted([f for f in output_dir.glob("*ì›Œí¬ë¶.html") if "í•©ë³¸" not in f.name])
    if len(html_files) < 2:
        return
    
    print(f"\nğŸ“ HTML í•©ì¹˜ê¸°: {len(html_files)}ê°œ íŒŒì¼...")
    
    # íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œí•˜ì—¬ í•©ë³¸ëª… ìƒì„±
    import re as _re
    titles = []
    for hf in html_files:
        # "3ê³¼_03ê°•_02ë²ˆ_ì›Œí¬ë¶.html" â†’ "03ê°• 02ë²ˆ"
        m = _re.search(r'(\d+ê°•)[_ ](\w+)', hf.stem.replace('_ì›Œí¬ë¶',''))
        if m:
            titles.append(f"{m.group(1)} {m.group(2)}")
    
    if titles:
        first = titles[0]   # ì˜ˆ: "03ê°• 01ë²ˆ"
        last = titles[-1]   # ì˜ˆ: "04ê°• 04ë²ˆ"
        merge_name = f"{first}~{last} í•©ë³¸.html"
    else:
        merge_name = "ì „ì²´_ì›Œí¬ë¶_í•©ë³¸.html"
    
    # ì²« íŒŒì¼ì—ì„œ CSS ì¶”ì¶œ
    first_html = html_files[0].read_text(encoding='utf-8')
    style_match = _re.search(r'<style[^>]*>(.*?)</style>', first_html, _re.DOTALL)
    css = style_match.group(1) if style_match else ""
    
    # ê° íŒŒì¼ì—ì„œ <body> ë‚´ìš©ë§Œ ì¶”ì¶œ
    all_bodies = []
    for hf in html_files:
        html = hf.read_text(encoding='utf-8')
        body_match = _re.search(r'<body[^>]*>(.*?)</body>', html, _re.DOTALL)
        if body_match:
            all_bodies.append(body_match.group(1))
    
    # í•©ì¹œ HTML ìƒì„±
    merged_path = _unique_path(output_dir, merge_name.replace('.html', ''), '.html')
    merged = f"""<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="UTF-8">
<title>{merged_path.stem}</title>
<style>
{css}
</style>
</head>
<body>
{''.join(all_bodies)}
</body>
</html>"""
    
    merged_path.write_text(merged, encoding='utf-8')
    print(f"  âœ… í•©ë³¸ ìƒì„±: {merged_path.name}")
    print(f"  â„¹ï¸  í¬ë¡¬ì—ì„œ ì—´ê³  Ctrl+P â†’ PDFë¡œ ì €ì¥í•˜ì„¸ìš” (í•œë²ˆì— ì „ë¶€!)")
if __name__ == "__main__":
    # --level íŒŒì‹± (ì–´ë””ì„œë“  ì‚¬ìš© ê°€ëŠ¥)
    levels = None
    filtered_args = []
    for i, arg in enumerate(sys.argv):
        if arg == "--level" and i+1 < len(sys.argv):
            levels = [int(x) for x in sys.argv[i+1].split(",")]
            continue
        if i > 0 and sys.argv[i-1] == "--level":
            continue
        filtered_args.append(arg)
    sys.argv = filtered_args

    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•:")
        print("  ì—¬ëŸ¬ ì§€ë¬¸: py pipeline.py --all all.txt")
        print("  ë ˆë²¨ ì„ íƒ: py pipeline.py --all all.txt --level 1,2,5,8")
        print("  1ê°œ ì§€ë¬¸:  py pipeline.py ì§€ë¬¸.txt 5 \"05ê°• 01ë²ˆ\"")
        print("  HTML í•©ì¹˜ê¸°: py pipeline.py --merge")
        print()
        print("  --level ì˜µì…˜: ì›í•˜ëŠ” ë ˆë²¨ë§Œ ì¶œë ¥ (0=í‘œì§€+ì •ë‹µ)")
        print("    ì˜ˆ) --level 1,2,3,4    â†’ Lv.1~4ë§Œ")
        print("    ì˜ˆ) --level 5,6,7,8    â†’ Lv.5~8ë§Œ")
        print("    ì˜ˆ) --level 0,1,2      â†’ í‘œì§€+Lv.1+Lv.2")
        sys.exit(1)

    if levels:
        print(f"ğŸ“‹ ë ˆë²¨ í•„í„°: Lv.{','.join(str(l) for l in levels)}")

    if sys.argv[1] == "--merge":
        merge_html_files()
    elif sys.argv[1] == "--all":
        filepath = sys.argv[2]
        lesson = sys.argv[3] if len(sys.argv) > 3 else "5"
        split_and_run(filepath, lesson, levels=levels)
    elif sys.argv[1] == "--batch":
        with open(sys.argv[2], 'r', encoding='utf-8') as f:
            batch = json.load(f)
        process_batch(batch, levels=levels)
    else:
        passage_file = sys.argv[1]
        with open(passage_file, 'r', encoding='utf-8') as f:
            passage = f.read().strip()
        lesson_num = sys.argv[2] if len(sys.argv) > 2 else "1"
        title = sys.argv[3] if len(sys.argv) > 3 else Path(passage_file).stem
        meta = {
            "subject": "ìˆ˜íŠ¹ ì˜ì–´", "publisher": "EBS",
            "lesson_num": lesson_num, "lesson_n": lesson_num,
            "challenge_title": title
        }
        process_passage(passage, meta, f"passage_{lesson_num}_{title}", levels=levels)
