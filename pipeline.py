#!/usr/bin/env python3
"""Workbook generation pipeline"""
PIPELINE_VERSION = "v9-curl-final"
import asyncio, json, os, sys, time, random, re
from pathlib import Path

# ============================================================
# ì„¤ì •
# ============================================================
API_KEY = os.environ.get("ANTHROPIC_API_KEY", "")
MODEL = "claude-sonnet-4-20250514"
TEMPLATE_DIR = Path(__file__).parent
DATA_DIR = TEMPLATE_DIR / "data"

# date-based output folder
from datetime import datetime
try:
    TODAY = datetime.now().strftime("%-mì›”%-dì¼") if os.name != 'nt' else datetime.now().strftime("%#mì›”%#dì¼")
except:
    TODAY = datetime.now().strftime("%mì›”%dì¼")
OUTPUT_DIR = TEMPLATE_DIR / "output" / TODAY

# ============================================================
# Safe print (encoding-safe)
# ============================================================
def _safe_print(msg):
    try:
        print(str(msg))
    except Exception:
        pass

# ============================================================
# ë¬¸ì¥ ë¶„ë¦¬ (Dr. Mr. Ms. Mrs. Prof. etc. ê²½ì¹­ ë³´í˜¸)
# ============================================================
# ë§ˆì¹¨í‘œ ë’¤ ê³µë°±ì—ì„œ ë¶„ë¦¬í•˜ë˜, ê²½ì¹­/ì•½ì–´ ë’¤ëŠ” ë¶„ë¦¬í•˜ì§€ ì•ŠìŒ
_ABBREVS = r'(?<!\bDr)(?<!\bMr)(?<!\bMs)(?<!\bSt)(?<!\bvs)(?<!\bNo)(?<!\bJr)(?<!\bSr)(?<!\bet)(?<!\bMrs)(?<!\bal)(?<!\bProf)(?<!\bGen)(?<!\bGov)(?<!\bSgt)(?<!\bCpl)(?<!\bLt)(?<!\bCo)(?<!\bInc)(?<!\bLtd)(?<!\bCorp)(?<!\bDept)(?<!\bEst)(?<!\bFig)(?<!\bVol)(?<!\bRev)'

def split_sentences(text: str) -> list:
    """ì˜ì–´ ì§€ë¬¸ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ê²½ì¹­/ì•½ì–´ ë§ˆì¹¨í‘œ ë³´í˜¸)"""
    # 1ë‹¨ê³„: ê²½ì¹­/ì•½ì–´ì˜ ë§ˆì¹¨í‘œë¥¼ ì„ì‹œ í† í°ìœ¼ë¡œ ì¹˜í™˜ (ë‹¨ì–´ ê²½ê³„ ê¸°ë°˜)
    protected = text
    abbrevs = [
        'Dr.', 'Mr.', 'Ms.', 'Mrs.', 'Prof.', 'Jr.', 'Sr.', 'St.',
        'vs.', 'etc.', 'No.', 'Vol.', 'Fig.', 'Gen.', 'Gov.', 'Rev.',
        'Sgt.', 'Cpl.', 'Lt.', 'Co.', 'Inc.', 'Ltd.', 'Corp.', 'Dept.',
        'Est.', 'al.', 'e.g.', 'i.e.', 'U.S.', 'U.K.', 'U.N.',
    ]
    replacements = {}
    for ab in abbrevs:
        token = ab.replace('.', 'Â§DOTÂ§')
        # ë‹¨ì–´ ê²½ê³„(\b)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ì•½ì–´ë§Œ ë§¤ì¹˜
        # ì˜ˆ: 'al.'ì€ ë‹¨ë… ë‹¨ì–´ì¼ ë•Œë§Œ (et al.), 'meal.'ì˜ 'al.'ì€ ë§¤ì¹˜ ì•ˆ ë¨
        pattern = r'(?<!\w)' + re.escape(ab)
        if re.search(pattern, protected):
            replacements[token] = ab
            protected = re.sub(pattern, token, protected)
    
    # 2ë‹¨ê³„: ì¼ë°˜ ë¬¸ì¥ ë¶„ë¦¬ (ë‹«ëŠ” ë”°ì˜´í‘œ ë’¤ ëŒ€ë¬¸ì ì¼ë°˜ë¬¸ì¥ë§Œ ë¶„ë¦¬, ë”°ì˜´í‘œ ì—°ì†ì€ í•©ì¹¨)
    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+(?=[A-Z])(?!["\u201c])|(?<=[.!?]["\u201d])\s+(?=[A-Z])(?!["\u201c])', protected) if s.strip()]
    
    # 3ë‹¨ê³„: í† í°ì„ ì›ë˜ ë§ˆì¹¨í‘œë¡œ ë³µì›
    restored = []
    for s in sentences:
        for token, original in replacements.items():
            s = s.replace(token, original)
        restored.append(s)
    
    return restored

# ============================================================
# Claude API call (curl subprocess - ONLY method that bypasses Python latin-1)
# ============================================================
API_URL = "https://api.anthropic.com/v1/messages"

def call_claude(system_prompt: str, user_prompt: str, max_retries=2, max_tokens=4096) -> str:
    """Claude API via curl subprocess - zero Python encoding involvement"""
    import subprocess, tempfile
    if not API_KEY:
        raise ValueError("ANTHROPIC_API_KEY not set")
    
    body = {
        "model": MODEL,
        "max_tokens": max_tokens,
        "temperature": 0.3,
        "system": system_prompt,
        "messages": [{"role": "user", "content": user_prompt}]
    }
    body_json = json.dumps(body, ensure_ascii=False)
    
    for attempt in range(max_retries + 1):
        tmp_path = None
        try:
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', encoding='utf-8', delete=False) as tmp:
                tmp.write(body_json)
                tmp_path = tmp.name
            
            result = subprocess.run(
                [
                    'curl', '-s', '-X', 'POST', API_URL,
                    '-H', f'x-api-key: {API_KEY}',
                    '-H', 'anthropic-version: 2023-06-01',
                    '-H', 'content-type: application/json; charset=utf-8',
                    '-d', f'@{tmp_path}'
                ],
                capture_output=True,
                timeout=120
            )
            
            if tmp_path:
                try: os.unlink(tmp_path)
                except: pass
            
            if result.returncode != 0:
                raise Exception(f"curl error: {result.stderr.decode('utf-8','replace')[:200]}")
            
            data = json.loads(result.stdout.decode('utf-8'))
            if 'error' in data:
                raise Exception(f"API error: {json.dumps(data['error'])[:200]}")
            return data["content"][0]["text"].strip()
        except Exception as e:
            if tmp_path:
                try: os.unlink(tmp_path)
                except: pass
            _safe_print(f"  [WARN] API attempt {attempt+1} failed: {str(e)[:100]}")
            if attempt < max_retries:
                time.sleep(3 * (attempt + 1))
            else:
                raise

def call_claude_json(system_prompt: str, user_prompt: str, max_retries=3, max_tokens=4096) -> dict:
    """Claude API call -> JSON parse with retry"""
    last_error = None
    for attempt in range(max_retries + 1):
        try:
            text = call_claude(system_prompt, user_prompt, max_retries=0, max_tokens=max_tokens)
            return _parse_json_robust(text)
        except (json.JSONDecodeError, ValueError) as e:
            last_error = e
            try:
                _safe_print(f"  [WARN] JSON parse fail (try {attempt+1}/{max_retries+1}): {str(e)[:80]}")
            except Exception:
                pass
            if attempt < max_retries:
                time.sleep(2)
    raise ValueError(f"JSON parse final fail: {str(last_error)[:200]}")

def _parse_json_robust(text: str) -> dict:
    """ì—¬ëŸ¬ ì „ëµìœ¼ë¡œ JSON íŒŒì‹± ì‹œë„"""
    # 1) ì½”ë“œë¸”ë¡ ì œê±°
    text = re.sub(r'^```json\s*', '', text.strip())
    text = re.sub(r'\s*```$', '', text.strip())
    text = text.strip()
    
    # 2) ì§ì ‘ íŒŒì‹±
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    
    # 3) JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ê°€ì¥ ë°”ê¹¥ { } ë§¤ì¹­)
    match = re.search(r'\{[\s\S]*\}', text)
    if match:
        try:
            return json.loads(match.group())
        except json.JSONDecodeError:
            pass
    
    # 4) ì´ìŠ¤ì¼€ì´í”„ ì•ˆ ëœ ë”°ì˜´í‘œ ìˆ˜ì •: value ì•ˆì˜ " â†’ \"
    try:
        fixed = _fix_json_quotes(text if not match else match.group())
        return json.loads(fixed)
    except (json.JSONDecodeError, Exception):
        pass
    
    # 5) ì¤„ë°”ê¿ˆ/íƒ­ ì´ìŠ¤ì¼€ì´í”„
    try:
        cleaned = text if not match else match.group()
        # JSON ë¬¸ìì—´ ì•ˆì˜ ì‹¤ì œ ì¤„ë°”ê¿ˆì„ \nìœ¼ë¡œ ë³€í™˜
        cleaned = re.sub(r'(?<=": ")([^"]*?)(?=")', lambda m: m.group(1).replace('\n', '\\n').replace('\t', '\\t'), cleaned)
        return json.loads(cleaned)
    except (json.JSONDecodeError, Exception):
        pass
    
    raise json.JSONDecodeError("ëª¨ë“  íŒŒì‹± ì „ëµ ì‹¤íŒ¨", text[:200], 0)

def _fix_json_quotes(text: str) -> str:
    """JSON ë¬¸ìì—´ ì•ˆì˜ ì´ìŠ¤ì¼€ì´í”„ ì•ˆ ëœ ë”°ì˜´í‘œë¥¼ ìˆ˜ì •"""
    result = []
    in_string = False
    escape_next = False
    for i, ch in enumerate(text):
        if escape_next:
            result.append(ch)
            escape_next = False
            continue
        if ch == '\\':
            result.append(ch)
            escape_next = True
            continue
        if ch == '"':
            if not in_string:
                in_string = True
                result.append(ch)
            else:
                # ë‹¤ìŒ ë¬¸ì í™•ì¸: , } ] : ê³µë°±ì´ë©´ ë¬¸ìì—´ ë
                rest = text[i+1:i+10].lstrip()
                if not rest or rest[0] in ',}]:':
                    in_string = False
                    result.append(ch)
                else:
                    result.append('\\"')  # ì´ìŠ¤ì¼€ì´í”„
                    continue
        else:
            result.append(ch)
    return ''.join(result)

# ============================================================
# ë‹¨ê³„ë³„ ì €ì¥/ë¡œë“œ
# ============================================================
def _run_async(coro):
    """Run an async coroutine from sync context (pipeline runs in a thread)."""
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None
    if loop and loop.is_running():
        # We're inside an async event loop (FastAPI thread) â€“ schedule as a task
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as pool:
            return pool.submit(asyncio.run, coro).result()
    else:
        return asyncio.run(coro)

def save_step(passage_dir: Path, step_name: str, data: dict):
    # Save locally
    passage_dir.mkdir(parents=True, exist_ok=True)
    path = passage_dir / f"{step_name}.json"
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    _safe_print(f"  Saved: {step_name}.json")
    # Save to Supabase
    try:
        import supa
        if supa._enabled():
            cache_key = passage_dir.name
            _run_async(supa.save_step_supa(cache_key, step_name, data))
            _safe_print(f"  Saved to Supabase: {cache_key}/{step_name}")
    except Exception as e:
        _safe_print(f"  [supa] save error: {str(e)[:80]}")

def load_step(passage_dir: Path, step_name: str) -> dict | None:
    # Try local first
    path = passage_dir / f"{step_name}.json"
    if path.exists():
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    # Try Supabase
    try:
        import supa
        if supa._enabled():
            cache_key = passage_dir.name
            data = _run_async(supa.get_step(cache_key, step_name))
            if data:
                # Save locally for future use
                passage_dir.mkdir(parents=True, exist_ok=True)
                with open(path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                _safe_print(f"  Loaded from Supabase: {step_name}")
                return data
    except Exception as e:
        _safe_print(f"  [supa] load error: {str(e)[:80]}")
    return None

# ============================================================
# SYSTEM PROMPT (ê³µí†µ)
# ============================================================
SYS_JSON = """You are an English exam content generator for Korean high school students.
Return ONLY valid JSON. No markdown fences. No explanations. No preamble.
All Korean text must use proper Korean. All English must be grammatically correct."""

SYS_JSON_KR = """ë‹¹ì‹ ì€ í•œêµ­ ê³ ë“±í•™ìƒì„ ìœ„í•œ ì˜ì–´ ì‹œí—˜ ì½˜í…ì¸  ìƒì„±ê¸°ì…ë‹ˆë‹¤.
ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”. ë§ˆí¬ë‹¤ìš´, ì„¤ëª…, ì„œë¬¸ ì—†ì´ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
í•œêµ­ì–´ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ, ì˜ì–´ëŠ” ë¬¸ë²•ì ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."""

# ============================================================
# STEP 1: ê¸°ë³¸ ë¶„ì„ (ì–´íœ˜ + ë²ˆì—­ + í•µì‹¬ë¬¸ì¥)
# ============================================================
def step1_basic_analysis(passage: str, passage_dir: Path) -> dict:
    cached = load_step(passage_dir, "step1_basic")
    if cached:
        _safe_print("  step1: using cache")
        return cached

    sentences_regex = split_sentences(passage)
    sent_count = len(sentences_regex)

    _safe_print("  step1: basic analysis...")
    prompt = f"""ë‹¤ìŒ ì˜ì–´ ì§€ë¬¸ì„ ë¶„ì„í•˜ì—¬ JSONì„ ìƒì„±í•˜ì„¸ìš”.

[ì§€ë¬¸ - ì´ {sent_count}ê°œ ë¬¸ì¥]
{passage}

[ìƒì„± í•­ëª©]
1. vocab: í•µì‹¬ ì–´íœ˜ 14ê°œ (ê°ê° word, meaning(í•œêµ­ì–´), synonyms(ì˜ì–´ ë™ì˜ì–´ 4ê°œ ì‰¼í‘œêµ¬ë¶„))
2. translation: ì§€ë¬¸ ì „ì²´ì˜ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë²ˆì—­
3. sentences: ì§€ë¬¸ì˜ ëª¨ë“  ë¬¸ì¥ì„ ê°œë³„ ë°°ì—´ë¡œ ë¶„ë¦¬ (ì •í™•íˆ {sent_count}ê°œ!)
   - ì§§ì€ ë¬¸ì¥ë„ ì ˆëŒ€ í•©ì¹˜ì§€ ë§ˆì„¸ìš” (ì˜ˆ: "That's not loyalty." ëŠ” ë…ë¦½ ë¬¸ì¥)
   - ë¬¸ì¥ì„ ì ˆëŒ€ ë¶„ë¦¬í•˜ì§€ ë§ˆì„¸ìš” (ì„¸ë¯¸ì½œë¡  ; ìœ¼ë¡œ ì—°ê²°ëœ ê²ƒì€ 1ë¬¸ì¥)
4. sentence_translations: ê° ë¬¸ì¥ì˜ í•œêµ­ì–´ ë²ˆì—­ (sentencesì™€ ì •í™•íˆ ê°™ì€ ìˆ˜, ê°™ì€ ìˆœì„œ!)
5. key_sentences: ì‹œí—˜ ì¶œì œ ê°€ëŠ¥ì„±ì´ ë†’ì€ í•µì‹¬ ë¬¸ì¥ 8ê°œ (ì›ë¬¸ ê·¸ëŒ€ë¡œ)
6. test_a: vocabì—ì„œ ëœ» ì“°ê¸° í…ŒìŠ¤íŠ¸ìš© 5ê°œ ë‹¨ì–´ (ì˜ì–´)
7. test_b: vocabì—ì„œ ë™ì˜ì–´ í…ŒìŠ¤íŠ¸ìš© 5ê°œ ë‹¨ì–´ (test_aì™€ ê²¹ì¹˜ì§€ ì•Šê²Œ, ì˜ì–´)
8. test_c: vocabì—ì„œ ì² ì í…ŒìŠ¤íŠ¸ìš© 5ê°œ (í•œêµ­ì–´ ëœ»)

JSON í˜•ì‹:
{{
  "vocab": [{{"word":"...", "meaning":"...", "synonyms":"..."}}],
  "translation": "...",
  "sentences": ["...", "..."],
  "sentence_translations": ["ì²«ì§¸ ë¬¸ì¥ í•´ì„...", "ë‘˜ì§¸ ë¬¸ì¥ í•´ì„...", ...],
  "key_sentences": ["...", "..."],
  "test_a": ["...", "..."],
  "test_b": ["...", "..."],
  "test_c": ["...", "..."]
}}"""

    data = call_claude_json(SYS_JSON_KR, prompt, max_tokens=4096)
    
    # ğŸ”’ ê²€ì¦: API ë¬¸ì¥ ë¶„ë¦¬ ëŒ€ì‹  í•­ìƒ regex ì‚¬ìš© (AIê°€ ë¬¸ì¥ì„ í•©ì¹˜ê±°ë‚˜ ìª¼ê°œëŠ” ê²ƒ ë°©ì§€)
    data["sentences"] = sentences_regex
    _safe_print(f"  Sentence count: {sent_count}")
    
    save_step(passage_dir, "step1_basic", data)
    return data

# ============================================================
# ìˆœì„œ ì„ ì§€ ì½”ë“œ ìƒì„± ìœ í‹¸ë¦¬í‹°
# ============================================================
_CIRCLE_NUMS = ["â‘ ","â‘¡","â‘¢","â‘£","â‘¤","â‘¥","â‘¦","â‘§","â‘¨","â‘©"]

def _generate_order_choices(data):
    """
    1) order_paragraphs (A)(B)(C) ë¼ë²¨ì„ ì…”í”Œ â†’ ì •ë‹µì´ í•­ìƒ ABCê°€ ì•„ë‹ˆê²Œ
    2) order_choices 5ì§€ì„ ë‹¤ë¥¼ ì½”ë“œë¡œ ìƒì„±
    3) full_order_blocks ìˆœì„œë„ ì…”í”Œ
    """
    from itertools import permutations
    
    # === 1. 3ë‹¨ë½ ë¼ë²¨ ì…”í”Œ ===
    paras = data.get("order_paragraphs", [])
    if len(paras) == 3:
        # í˜„ì¬: [[A, text1], [B, text2], [C, text3]] (ì›ë¬¸ ìˆœì„œ)
        # ì›ë¬¸ ìˆœì„œ ê¸°ì–µ (ì¸ë±ìŠ¤ 0,1,2 = ì •ë‹µ ìˆœì„œ)
        labels = ["A", "B", "C"]
        random.shuffle(labels)
        # ìƒˆ ë¼ë²¨ ë¶€ì—¬: ì²«ë²ˆì§¸ ë‹¨ë½ â†’ labels[0], ë‘ë²ˆì§¸ â†’ labels[1], ...
        new_paras = [[labels[i], paras[i][1]] for i in range(3)]
        # ì •ë‹µ = labelsë¥¼ ì›ë˜ ìˆœì„œëŒ€ë¡œ ì½ì€ ê²ƒ (labels[0] â†’ labels[1] â†’ labels[2])
        correct = tuple(labels)  # ì˜ˆ: ("C", "A", "B") = ì •ë‹µ
        # í‘œì‹œí•  ë•ŒëŠ” ë¼ë²¨ ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬
        new_paras.sort(key=lambda x: x[0])
        data["order_paragraphs"] = new_paras
    else:
        correct = ("A", "B", "C")
    
    # === 2. ì„ ì§€ 5ê°œ ìƒì„± ===
    all_perms = list(permutations(["A", "B", "C"]))
    wrong = [p for p in all_perms if p != correct]
    selected_wrong = random.sample(wrong, 4)
    all_choices = [correct] + selected_wrong
    random.shuffle(all_choices)
    
    choices = []
    answer = ""
    for i, perm in enumerate(all_choices):
        text = f"({perm[0]})-({perm[1]})-({perm[2]})"
        choices.append(f"{_CIRCLE_NUMS[i]} {text}")
        if perm == correct:
            answer = f"{_CIRCLE_NUMS[i]} {text}"
    data["order_choices"] = choices
    data["order_answer"] = answer
    
    # === 3. ì „ì²´ ë¬¸ì¥ ë°°ì—´ (ì‹¬í™”) ì…”í”Œ ===
    blocks = data.get("full_order_blocks", [])
    if len(blocks) >= 2:
        # ì›ë¬¸ ìˆœì„œ ê¸°ì–µ (ì •ë‹µ)
        original_labels = [b[0] for b in blocks]
        # ìƒˆ ë¼ë²¨ ë¶€ì—¬ + ì…”í”Œ
        n = len(blocks)
        alpha = [chr(65+i) for i in range(n)]  # A, B, C, D, E, ...
        random.shuffle(alpha)
        # ê° ì›ë¬¸ ë¬¸ì¥ì— ìƒˆ ë¼ë²¨
        new_blocks = [[alpha[i], blocks[i][1]] for i in range(n)]
        # ì •ë‹µ ìˆœì„œ = alpha[0] â†’ alpha[1] â†’ ... (ì›ë¬¸ ìˆœì„œëŒ€ë¡œ ë¼ë²¨ ì½ê¸°)
        correct_order = "â†’".join([f"({alpha[i]})" for i in range(n)])
        data["full_order_answer"] = correct_order
        # í‘œì‹œëŠ” ë¼ë²¨ ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬ (ì…”í”Œ íš¨ê³¼!)
        new_blocks.sort(key=lambda x: x[0])
        data["full_order_blocks"] = new_blocks

# ============================================================
# STEP 2: Lv.5 ìˆœì„œ/ì‚½ì…
# ============================================================
def step2_order(passage: str, sentences: list, passage_dir: Path) -> dict:
    cached = load_step(passage_dir, "step2_order")
    if cached:
        _safe_print("  step2: using cache")
        return cached

    _safe_print("  step2: generating Lv.5 order...")
    prompt = f"""ë‹¤ìŒ ì˜ì–´ ì§€ë¬¸ìœ¼ë¡œ ìˆœì„œ ë°°ì—´ + ë¬¸ì¥ ì‚½ì… ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

[ì§€ë¬¸]
{passage}

[ê°œë³„ ë¬¸ì¥]
{json.dumps(sentences, ensure_ascii=False)}

[ìƒì„± í•­ëª©]
1. order_intro: ì œì‹œë¬¸ (ì²« 1~2ë¬¸ì¥)
2. order_paragraphs: (A)(B)(C) 3ê°œ ë‹¨ë½ (ê°ê° labelê³¼ text). ì •ë‹µ ìˆœì„œëŠ” ì›ë¬¸ ìˆœì„œëŒ€ë¡œ.
   - ëª¨ë“  ë¬¸ì¥ì´ ë¹ ì§ì—†ì´ í¬í•¨ë˜ì–´ì•¼ í•¨
3. order_choices: 5ì§€ì„ ë‹¤ (í˜•ì‹: "â‘  (A)-(C)-(B)" ë“±). ì •ë‹µ 1ê°œ í¬í•¨.
4. order_answer: ì •ë‹µ ë²ˆí˜¸ (ì˜ˆ: "â‘£ (C)-(A)-(B)")
5. insert_sentence: ì‚½ì…í•  ë¬¸ì¥ 1ê°œ (ì•ë’¤ ë¬¸ë§¥ ë‹¨ì„œê°€ ëª…í™•í•œ ê²ƒ)
6. insert_passage: ì‚½ì… ë¬¸ì¥ì„ ëº€ ë‚˜ë¨¸ì§€ ì§€ë¬¸ì— ( â‘  )~( â‘¤ ) ìœ„ì¹˜ í‘œì‹œ
7. insert_answer: ì‚½ì… ì •ë‹µ ë²ˆí˜¸
8. full_order_blocks: ì „ì²´ ë¬¸ì¥ì„ (A)~ëê¹Œì§€ ê°œë³„ ë¸”ë¡ìœ¼ë¡œ ë¶„í•  (ê°ê° label, text)
9. full_order_answer: ì •ë‹µ ìˆœì„œ (ì˜ˆ: "(C)â†’(G)â†’(D)â†’...")

JSON í˜•ì‹:
{{
  "order_intro": "...",
  "order_paragraphs": [{{"label":"A","text":"..."}}, ...],
  "order_choices": ["â‘  ...", "â‘¡ ...", ...],
  "order_answer": "...",
  "insert_sentence": "...",
  "insert_passage": "...",
  "insert_answer": "...",
  "full_order_blocks": [{{"label":"A","text":"..."}}, ...],
  "full_order_answer": "..."
}}"""

    data = call_claude_json(SYS_JSON, prompt, max_tokens=4096)
    # ë³€í™˜: order_paragraphsë¥¼ [label, text] í˜•íƒœë¡œ
    if data.get("order_paragraphs") and isinstance(data["order_paragraphs"][0], dict):
        data["order_paragraphs"] = [[p["label"], p["text"]] for p in data["order_paragraphs"]]
    if data.get("full_order_blocks") and isinstance(data["full_order_blocks"][0], dict):
        data["full_order_blocks"] = [[b["label"], b["text"]] for b in data["full_order_blocks"]]

    # â˜… ìˆœì„œ ì„ ì§€ë¥¼ ì½”ë“œë¡œ ì§ì ‘ ìƒì„± (AIê°€ ë‹¤ì–‘í•˜ê²Œ ì•ˆ ë§Œë“œëŠ” ë¬¸ì œ í•´ê²°)
    _generate_order_choices(data)

    # ğŸ”’ ê²€ì¦: ì „ì²´ë°°ì—´ ë¸”ë¡ ìˆ˜ vs ì›ë¬¸ ë¬¸ì¥ ìˆ˜
    block_count = len(data.get("full_order_blocks", []))
    sentence_count = len(sentences)
    if block_count != sentence_count:
        _safe_print(f"  WARNING: sentence mismatch! original {sentence_count} vs generated {block_count}, retrying...")
        # ìºì‹œ ì‚­ì œ í›„ ì¬ì‹œë„ (1íšŒ)
        cache_path = passage_dir / "step2_order.json"
        if cache_path.exists():
            cache_path.unlink()
        data = call_claude_json(SYS_JSON, prompt, max_tokens=4096)
        if data.get("order_paragraphs") and isinstance(data["order_paragraphs"][0], dict):
            data["order_paragraphs"] = [[p["label"], p["text"]] for p in data["order_paragraphs"]]
        if data.get("full_order_blocks") and isinstance(data["full_order_blocks"][0], dict):
            data["full_order_blocks"] = [[b["label"], b["text"]] for b in data["full_order_blocks"]]
        block_count2 = len(data.get("full_order_blocks", []))
        if block_count2 != sentence_count:
            _safe_print(f"  WARNING: still mismatch ({block_count2} vs {sentence_count}), using original")
            data["full_order_blocks"] = [[chr(65+i), s] for i, s in enumerate(sentences)]

    save_step(passage_dir, "step2_order", data)
    return data

# ============================================================
# STEP 3: Stage 6 ë¹ˆì¹¸ ì¶”ë¡ 
# ============================================================
def step3_blank(passage: str, passage_dir: Path) -> dict:
    cached = load_step(passage_dir, "step3_blank")
    if cached:
        _safe_print("  step3: using cache")
        return cached

    _safe_print("  step3: generating Lv.6 blanks...")
    prompt = f"""ë‹¤ìŒ ì˜ì–´ ì§€ë¬¸ìœ¼ë¡œ ë¹ˆì¹¸ ì¶”ë¡  ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

[ì§€ë¬¸]
{passage}

[ê·œì¹™]
- ì£¼ì œë¬¸(ê²°ë¡ ë¬¸)ì˜ í•µì‹¬ ë¶€ë¶„ì„ ë¹ˆì¹¸ìœ¼ë¡œ ë§Œë“¤ê¸°
- ë¹ˆì¹¸ì€ 15ë‹¨ì–´ ì´ë‚´ë¡œ (ë„ˆë¬´ ê¸´ ë¹ˆì¹¸ ê¸ˆì§€)
- ë¹ˆì¹¸ ë¬¸ì¥ ì™¸ì˜ ë‹¤ë¥¸ ë¬¸ì¥ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€ (ìƒëµ/ì¶•ì•½/ë³€í˜• ì ˆëŒ€ ê¸ˆì§€)
- ë¹ˆì¹¸ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë¬¸ì¥ ë¶€ë¶„ë„ ì ˆëŒ€ ë³€í˜•í•˜ì§€ ë§ ê²ƒ
- ì„ ì§€ 12ê°œ: ì •ë‹µ 6~7ê°œ + ì˜¤ë‹µ 5~6ê°œ
- ì •ë‹µ: ì›ë¬¸ í•µì‹¬ í‘œí˜„ì„ ë™ì˜ì–´/ë¹„ìœ ì  í‘œí˜„ìœ¼ë¡œ ë³€í˜•
- ì˜¤ë‹µ: ì§€ë¬¸ ë‚´ìš© ì™œê³¡, ë°˜ëŒ€ ì˜ë¯¸, ë¯¸ì–¸ê¸‰ ë‚´ìš©
- ê° ì„ ì§€ëŠ” 15ë‹¨ì–´ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ

[JSON í˜•ì‹]
{{
  "blank_passage": "ë¹ˆì¹¸ì´ í¬í•¨ëœ ì „ì²´ ì§€ë¬¸ (ë¹ˆì¹¸ì€ ____ë¡œ í‘œì‹œ)",
  "blank_answer_korean": "ë¹ˆì¹¸ ì •ë‹µ ë‚´ìš© í•œêµ­ì–´",
  "blank_options": ["â‘  ...", "â‘¡ ...", ... "â‘« ..."],
  "blank_correct": ["â‘¡", "â‘¢", "â‘¤", ...],
  "blank_wrong": ["â‘ ", "â‘£", ...]
}}"""

    data = call_claude_json(SYS_JSON, prompt, max_tokens=3000)
    save_step(passage_dir, "step3_blank", data)
    return data

# ============================================================
# STEP 4: Stage 7 ì£¼ì œ ì°¾ê¸°
# ============================================================
def step4_topic(passage: str, passage_dir: Path) -> dict:
    cached = load_step(passage_dir, "step4_topic")
    if cached:
        _safe_print("  step4: using cache")
        return cached

    _safe_print("  step4: generating Lv.7 topic...")
    prompt = f"""ë‹¤ìŒ ì˜ì–´ ì§€ë¬¸ìœ¼ë¡œ ì£¼ì œ ì°¾ê¸° ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

[ì§€ë¬¸]
{passage}

[ê·œì¹™]
- ì§€ë¬¸ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ (ìƒëµ/ë³€í˜• ê¸ˆì§€)
- ì„ ì§€ 12ê°œ: ì •ë‹µ 5ê°œ + ì˜¤ë‹µ 7ê°œ
- ì„ ì§€ëŠ” ë°˜ë“œì‹œ ì˜ì–´ë¡œ ì‘ì„± (í•œêµ­ì–´ ê¸ˆì§€)
- ì •ë‹µ: ì£¼ì œë¬¸ í‚¤ì›Œë“œë¥¼ ë™ì˜ì–´ë¡œ ì¹˜í™˜í•œ ì˜ì–´ í‘œí˜„
- ì˜¤ë‹µ: ì§€ë¬¸ ë¯¸ì–¸ê¸‰, ë¶€ë¶„ì  ë‚´ìš©, ì™œê³¡ (ì˜ì–´)
- ì¶”ë¡ ì  ì‚¬ê³  ê¸ˆì§€: ê¸€ì—ì„œ ì§ì ‘ ì–¸ê¸‰ëœ ë‚´ìš©ë§Œ ì •ë‹µ
- ê° ì„ ì§€ëŠ” 30ë‹¨ì–´ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ

[JSON í˜•ì‹]
{{
  "topic_passage": "ì›ë¬¸ ì „ë¬¸ (ê·¸ëŒ€ë¡œ)",
  "topic_options": ["â‘  the importance of...", "â‘¡ how to...", ... "â‘« ..."],
  "topic_correct": ["â‘¡", "â‘£", ...],
  "topic_wrong": ["â‘ ", "â‘¢", ...]
}}"""

    data = call_claude_json(SYS_JSON, prompt, max_tokens=3000)
    save_step(passage_dir, "step4_topic", data)
    return data

# ============================================================
# STEP 5: Lv.8 ì–´ë²•
# ============================================================
def step5_grammar(passage: str, passage_dir: Path) -> dict:
    cached = load_step(passage_dir, "step5_grammar")
    if cached:
        _safe_print("  step5: using cache")
        return cached

    sentences = split_sentences(passage)
    sent_count = len(sentences)
    error_count = min(8, sent_count)  # ë¬¸ì¥ ìˆ˜ë³´ë‹¤ ë§ì€ ì˜¤ë¥˜ ë¶ˆê°€
    bracket_count = min(20, sent_count * 2)  # ë¬¸ì¥ë‹¹ ìµœëŒ€ 2ê°œ ê´„í˜¸
    # bracket_count = sent_count * 2  # ë¬¸ì¥ë‹¹ ìµœëŒ€ 2ê°œ ê´„í˜¸ / ì˜ˆ: 12ë¬¸ì¥ì´ë©´ ìµœëŒ€ 24ê°œ ê´„í˜¸ ë¬¸ì œ + 24ê°œ ë‹µì•ˆ ë°•ìŠ¤ê°€ ìƒì„±ë©ë‹ˆë‹¤.
    
    _safe_print("  step5: generating Lv.8 grammar...")
    prompt = f"""ë‹¤ìŒ ì˜ì–´ ì§€ë¬¸ìœ¼ë¡œ ì–´ë²• ë¬¸ì œ 2ì¢…ë¥˜ë¥¼ ìƒì„±í•˜ì„¸ìš”.

[ì›ë¬¸ - ì´ {sent_count}ê°œ ë¬¸ì¥]
{passage}

[âš ï¸ ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™]
1. ì›ë¬¸ì€ ì •í™•íˆ {sent_count}ê°œ ë¬¸ì¥ì…ë‹ˆë‹¤
2. ì¶œë ¥ ì§€ë¬¸ë„ ë°˜ë“œì‹œ ì •í™•íˆ {sent_count}ê°œ ë¬¸ì¥ì´ì–´ì•¼ í•©ë‹ˆë‹¤
3. ì ˆëŒ€ ë¬¸ì¥ì„ ì¶”ê°€/ì‚­ì œ/ë¶„ë¦¬/í•©ì¹˜ê¸° í•˜ì§€ ë§ˆì„¸ìš”
4. ì›ë¬¸ ë¬¸ì¥ì— ê´„í˜¸ë‚˜ ì˜¤ë¥˜ë§Œ ì‚½ì…í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€
5. ë¬¸ì¥ ìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ ì˜¤ë¥˜/ê´„í˜¸ ìˆ˜ë¥¼ ì¤„ì´ì„¸ìš” (ë¬¸ì¥ ì¶”ê°€ëŠ” ì ˆëŒ€ ê¸ˆì§€!)

[ì–´ë²• ê´„í˜¸í˜• Lv.8-1]
- ì›ë¬¸ {sent_count}ê°œ ë¬¸ì¥ ëª¨ë‘ í¬í•¨ (ì¶œì œ ì•ˆ í•˜ëŠ” ë¬¸ì¥ë„ ì›ë¬¸ ê·¸ëŒ€ë¡œ)
- {bracket_count}ê°œ ê´„í˜¸: (N)[ì •ë‹µ / ì˜¤ë‹µ] í˜•íƒœ
- í•œ ë¬¸ì¥ì— ì—¬ëŸ¬ ê´„í˜¸ ê°€ëŠ¥
- ì¶œì œ: ì‹œì œ, ëŒ€ëª…ì‚¬, ë™ëª…ì‚¬, toë¶€ì •ì‚¬, í˜•ìš©ì‚¬/ë¶€ì‚¬, ê´€ê³„ëŒ€ëª…ì‚¬, ë¶„ì‚¬, ì‚¬ì—­ë™ì‚¬ ë“±

[ì–´ë²• ì„œìˆ í˜• Lv.8-2]
- ì›ë¬¸ {sent_count}ê°œ ë¬¸ì¥ ëª¨ë‘ í¬í•¨
- {error_count}ê°œ ë¬¸ë²• ì˜¤ë¥˜ ì‚½ì… (ë°‘ì¤„ ì—†ì´)
- í•œ ë¬¸ì¥ì— ìµœëŒ€ 1ê°œ ì˜¤ë¥˜
- ë¬¸ì¥ì´ {sent_count}ê°œë¿ì´ë¯€ë¡œ ì˜¤ë¥˜ë„ ìµœëŒ€ {error_count}ê°œë§Œ!

[JSON í˜•ì‹]
{{
  "grammar_bracket_passage": "ê´„í˜¸ í¬í•¨ ì „ì²´ ì§€ë¬¸ (ì •í™•íˆ {sent_count}ë¬¸ì¥)",
  "grammar_bracket_count": {bracket_count},
  "grammar_bracket_answers": [{{"num":1, "answer":"go", "wrong":"will go", "reason":"if ì¡°ê±´ì ˆ í˜„ì¬ì‹œì œ"}}, ...],
  "grammar_error_passage": "ì˜¤ë¥˜ í¬í•¨ ì „ì²´ ì§€ë¬¸ (ì •í™•íˆ {sent_count}ë¬¸ì¥)",
  "grammar_error_count": {error_count},
  "grammar_error_answers": [{{"num":1, "original":"watch", "error":"watching", "reason":"tend to + ë™ì‚¬ì›í˜•"}}, ...]
}}"""

    data = call_claude_json(SYS_JSON, prompt, max_tokens=4000)
    
    # ğŸ”’ ê²€ì¦: ë¬¸ì¥ ìˆ˜ ì²´í¬
    for key in ['grammar_bracket_passage', 'grammar_error_passage']:
        gen_text = data.get(key, '')
        gen_sents = len(split_sentences(gen_text))
        if gen_sents > sent_count + 1:
            _safe_print(f"  WARNING: {key}: {gen_sents} sentences (original {sent_count}), retrying...")
            cache_path = passage_dir / "step5_grammar.json"
            if cache_path.exists():
                cache_path.unlink()
            data = call_claude_json(SYS_JSON, prompt, max_tokens=4000)
            break
    
    save_step(passage_dir, "step5_grammar", data)
    return data

# ============================================================
# STEP 6: Lv.9 ì–´íœ˜ì‹¬í™” + ë‚´ìš©ì¼ì¹˜
# ============================================================
def step6_vocab_content(passage: str, passage_dir: Path) -> dict:
    cached = load_step(passage_dir, "step6_vocab_content")
    if cached:
        _safe_print("  step6: using cache")
        return cached

    _safe_print("  step6: generating Lv.9 vocab...")
    prompt = f"""ë‹¤ìŒ ì˜ì–´ ì§€ë¬¸ìœ¼ë¡œ ì–´íœ˜ ì‹¬í™” + ë‚´ìš© ì¼ì¹˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

[ì§€ë¬¸]
{passage}

[Lv.9-1 Part A ê·œì¹™]
- ì›ë¬¸ì˜ ëª¨ë“  ë¬¸ì¥ì„ ë¹ ì§ì—†ì´ í¬í•¨
- 7~9ê°œ ê´„í˜¸: (N)[ì •ë‹µ / ë°˜ì˜ì–´] í˜•íƒœ
- ì •ë‹µê³¼ ì˜¤ë‹µì€ ì˜ë¯¸ê°€ ë°˜ëŒ€ì¸ ë‹¨ì–´ ìŒìœ¼ë¡œ êµ¬ì„± (ì˜ˆ: regarded/overlooked, effective/futile, mild/severe, constant/intermittent)
- ë°œìŒ ìœ ì‚¬ ë‹¨ì–´ ì ˆëŒ€ ê¸ˆì§€. ë°˜ë“œì‹œ ë°˜ì˜ì–´ë¡œ ì¶œì œ
- ë¬¸ë§¥ì„ ì½ì–´ì•¼ ì •ë‹µì„ ê³ ë¥¼ ìˆ˜ ìˆëŠ” ìˆ˜ëŠ¥ ìˆ˜ì¤€ ë°˜ì˜ì–´ ìŒ

[Lv.9-1 Part B ê·œì¹™]
- 10ê°œ ë‹¨ì–´ (ìµœì†Œ 8ê°œ, ê°€ëŠ¥í•˜ë©´ 10ê°œ), ê° 5ê°œ ì„ íƒì§€
- 5ê°œ ì¤‘ ë™ì˜ì–´ 2ê°œ + ë°˜ì˜ì–´ 3ê°œë¡œ êµ¬ì„±
- "ëª¨ë‘ ê³ ë¥´ì‹œì˜¤" í˜•íƒœ: ë™ì˜ì–´ë§Œ ê³¨ë¼ì•¼ ì •ë‹µ
- ë™ì˜ì–´: ìˆ˜ëŠ¥ ìˆ˜ì¤€ì˜ ì •í™•í•œ ìœ ì˜ì–´
- ë°˜ì˜ì–´: í•´ë‹¹ ë‹¨ì–´ì™€ ì˜ë¯¸ê°€ ë°˜ëŒ€ì¸ ë‹¨ì–´ 3ê°œ
- ë°œìŒ/ì² ì ìœ ì‚¬ ë‹¨ì–´ ì ˆëŒ€ ê¸ˆì§€. ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œë§Œ ì¶œì œ

[ë‚´ìš© ì¼ì¹˜ ê·œì¹™]
- content_match_kr: ë°˜ë“œì‹œ 10ê°œ í•œêµ­ì–´ ì„ ì§€ (â‘ ~â‘©). ì¼ì¹˜ 3~5ê°œ + ë¶ˆì¼ì¹˜ 5~7ê°œ
- content_match_en: ë°˜ë“œì‹œ 10ê°œ ì˜ì–´ ì„ ì§€ (â‘ ~â‘©). ì¼ì¹˜ 3~5ê°œ + ë¶ˆì¼ì¹˜ 5~7ê°œ
- í•œêµ­ì–´ì™€ ì˜ì–´ ì„ ì§€ì˜ ìˆœì„œëŠ” ì„œë¡œ ë‹¤ë¥´ê²Œ ëœë¤ ë°°ì¹˜

[JSON í˜•ì‹]
{{
  "vocab_advanced_passage": "ê´„í˜¸ í¬í•¨ ì§€ë¬¸",
  "vocab_parta_answers": [{{"num":1, "answer":"regarded", "wrong":"overlooked", "reason":"~ë¡œ ì—¬ê²¨ì§€ë‹¤ vs ê°„ê³¼í•˜ë‹¤"}}, ...],
  "vocab_partb": [{{"word":"regarded", "choices":"considered / perceived / overlooked / neglected / dismissed"}}, ...],
  "vocab_partb_answers": [{{"num":1, "correct":["considered", "perceived"], "wrong":["overlooked", "neglected", "dismissed"]}}, ...],
  "content_match_kr": ["â‘  í‰ì†Œ ê·¹ì¥ì—ì„œ í˜¼ì ì˜í™”ë¥¼ ë³¸ë‹¤.", ... (ë°˜ë“œì‹œ 10ê°œ ì„ ì§€)],
  "content_match_kr_answer": ["â‘¡", "â‘¢", ...],
  "content_match_en": ["â‘  The writer normally watches movies alone.", ... (ë°˜ë“œì‹œ 10ê°œ ì„ ì§€)],
  "content_match_en_answer": ["â‘¡", "â‘£", ...]
}}"""

    data = call_claude_json(SYS_JSON_KR, prompt, max_tokens=4000)

    # í•œêµ­ì–´ ì„ ì§€ ì…”í”Œ
    kr_items = data.get("content_match_kr", [])
    kr_answers = set(data.get("content_match_kr_answer", []))
    if kr_items:
        kr_texts = [re.sub(r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]\s*', '', item) for item in kr_items]
        kr_correct = [_CIRCLE_NUMS[i] in kr_answers for i in range(len(kr_texts))]
        kr_pairs = list(zip(kr_texts, kr_correct))
        random.shuffle(kr_pairs)
        data["content_match_kr"] = [f"{_CIRCLE_NUMS[i]} {kr_pairs[i][0]}" for i in range(len(kr_pairs))]
        data["content_match_kr_answer"] = [_CIRCLE_NUMS[i] for i in range(len(kr_pairs)) if kr_pairs[i][1]]

    # Part B ì˜ì–´ ì„ ì§€ ì…”í”Œ (ë²ˆí˜¸ëŠ” ì˜¤ë¦„ì°¨ìˆœ ìœ ì§€, ë¬¸ì¥ë§Œ ëœë¤)
    en_items = data.get("content_match_en", [])
    en_answers = set(data.get("content_match_en_answer", []))
    if en_items:
        # ë²ˆí˜¸ì™€ ë¬¸ì¥ ë¶„ë¦¬
        texts = [re.sub(r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]\s*', '', item) for item in en_items]
        is_correct = [_CIRCLE_NUMS[i] in en_answers for i in range(len(texts))]
        # ë¬¸ì¥+ì •ë‹µ ìŒì„ ì…”í”Œ
        pairs = list(zip(texts, is_correct))
        random.shuffle(pairs)
        # ë²ˆí˜¸ ì¬ë¶€ì—¬ + ì •ë‹µ ê°±ì‹ 
        data["content_match_en"] = [f"{_CIRCLE_NUMS[i]} {pairs[i][0]}" for i in range(len(pairs))]
        data["content_match_en_answer"] = [_CIRCLE_NUMS[i] for i in range(len(pairs)) if pairs[i][1]]

    save_step(passage_dir, "step6_vocab_content", data)
    return data

# ============================================================
# STEP 7: Stage 10 ì˜ì‘ (API ë¶ˆí•„ìš” - í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ ì²˜ë¦¬)
# ============================================================
def step7_writing(sentences: list, translation: str, passage_dir: Path, sentence_translations: list = None) -> dict:
    cached = load_step(passage_dir, "step7_writing")
    if cached:
        _safe_print("  step7: using cache")
        return cached

    _safe_print("  step7: generating Lv.10 writing...")
    # í•œêµ­ì–´ ë¬¸ì¥: sentence_translations ìš°ì„ , ì—†ìœ¼ë©´ translation ë¶„ë¦¬
    if sentence_translations and len(sentence_translations) >= len(sentences):
        kr_sentences = sentence_translations
    else:
        kr_sentences = [s.strip() for s in re.split(r'(?<=[.!?ë‹¤ìš”ìŒì„])\s+', translation) if s.strip()]

    writing_items = []
    for i, eng in enumerate(sentences):
        words = eng.split()
        # ëŒ€ë¬¸ìâ†’ì†Œë¬¸ì ë³€í™˜ (ì²« ë‹¨ì–´, I/ê³ ìœ ëª…ì‚¬ ì œì™¸)
        processed = []
        for j, w in enumerate(words):
            if j == 0 and w[0].isupper() and w not in ['I', 'I,']:
                # ê³ ìœ ëª…ì‚¬ ì²´í¬ (ê°„ë‹¨íˆ: 2ê¸€ì ì´ìƒ ëŒ€ë¬¸ì ì‹œì‘)
                if not (len(w) > 1 and w[1:].islower() and w[0].isupper() and any(c.isupper() for c in w)):
                    w = w[0].lower() + w[1:]
            processed.append(w)
        # ë§ˆì¹¨í‘œ/ëŠë‚Œí‘œ/ë¬¼ìŒí‘œ ì œê±°
        last = processed[-1]
        if last.endswith(('.', '!', '?')):
            processed[-1] = last[:-1]
        # ì…”í”Œ
        shuffled = processed.copy()
        random.shuffle(shuffled)
        scrambled = ' / '.join(shuffled)

        kr = kr_sentences[i] if i < len(kr_sentences) else f"ë¬¸ì¥ {i+1}"
        writing_items.append({
            "korean": kr,
            "scrambled": scrambled,
            "answer": eng
        })

    data = {"writing_items": writing_items}
    save_step(passage_dir, "step7_writing", data)
    return data

# ============================================================
# STEP 8: ì •ë‹µ ìƒì„±
# ============================================================
def step8_answers(all_data: dict, passage_dir: Path) -> dict:
    cached = load_step(passage_dir, "step8_answers")
    if cached:
        _safe_print("  step8: using cache")
        return cached

    _safe_print("  step8: generating answer page...")
    # ì •ë‹µ HTML ìƒì„± (ë ˆë²¨ë³„ ë¸”ë¡í™”)
    blocks = []

    # Lv.1
    blocks.append('<div class="ablock"><p class="ast">Stage 1 ì–´íœ˜ í…ŒìŠ¤íŠ¸</p>'
                   '<p>A. (ì–´íœ˜ í…ŒìŠ¤íŠ¸ ì •ë‹µì€ í•™ìƒì´ ì§ì ‘ í™•ì¸)</p></div>')

    # Lv.5
    s2 = all_data.get("step2", {})
    blocks.append(f'<div class="ablock"><p class="ast">Stage 5 ìˆœì„œ ë°°ì—´</p>'
                   f'<p>ì •ë‹µ: {s2.get("order_answer","")}</p>'
                   f'<p>ì‚½ì… ì •ë‹µ: {s2.get("insert_answer","")}</p>'
                   f'<p>ì „ì²´ ë°°ì—´: {s2.get("full_order_answer","")}</p></div>')

    # Lv.6
    s3 = all_data.get("step3", {})
    correct = ', '.join(s3.get("blank_correct", []))
    blocks.append(f'<div class="ablock"><p class="ast">Stage 6 ë¹ˆì¹¸ ì¶”ë¡ </p>'
                   f'<p>ì •ë‹µ: {correct}</p></div>')

    # Lv.7
    s4 = all_data.get("step4", {})
    correct = ', '.join(s4.get("topic_correct", []))
    blocks.append(f'<div class="ablock"><p class="ast">Stage 7 ì£¼ì œ ì°¾ê¸°</p>'
                   f'<p>ì •ë‹µ: {correct}</p></div>')

    # Lv.8 ê´„í˜¸
    s5 = all_data.get("step5", {})
    lv8_bracket = ['<div class="ablock"><p class="ast">Stage 8 ì–´ë²• (ê´„í˜¸)</p>']
    for a in s5.get("grammar_bracket_answers", []):
        if isinstance(a, dict):
            lv8_bracket.append(f'<p>({a.get("num","")}) {a.get("answer","")}</p>')
    lv8_bracket.append('</div>')
    blocks.append(''.join(lv8_bracket))

    # Stage 8 ì„œìˆ í˜•
    lv8_error = ['<div class="ablock"><p class="ast">Stage 8 ì„œìˆ í˜•</p>']
    for a in s5.get("grammar_error_answers", []):
        if isinstance(a, dict):
            lv8_error.append(f'<p>{a.get("error","")}->{a.get("original","")}({a.get("reason","")})</p>')
    lv8_error.append('</div>')
    blocks.append(''.join(lv8_error))

    # Lv.9-1 Part A
    s6 = all_data.get("step6", {})
    lv9a = ['<div class="ablock"><p class="ast">Stage 9-1 ì–´íœ˜ Part A</p>']
    for a in s6.get("vocab_parta_answers", []):
        if isinstance(a, dict):
            lv9a.append(f'<p>({a.get("num","")}) {a.get("answer","")}</p>')
    lv9a.append('</div>')
    blocks.append(''.join(lv9a))

    # Lv.9-1 Part B + Lv.9-2
    lv9b = ['<div class="ablock"><p class="ast">Stage 9-1 ì–´íœ˜ Part B</p>']
    for a in s6.get("vocab_partb_answers", []):
        if isinstance(a, dict):
            correct_list = ', '.join(a.get("correct", []))
            lv9b.append(f'<p>{a.get("num","")}: {correct_list}</p>')
    kr_ans = ', '.join(s6.get("content_match_kr_answer", []))
    en_ans = ', '.join(s6.get("content_match_en_answer", []))
    lv9b.append(f'<p class="ast">Lv.9-2 ë‚´ìš©ì¼ì¹˜</p>')
    lv9b.append(f'<p>í•œêµ­ì–´: {kr_ans}</p>')
    lv9b.append(f'<p>ì˜ì–´: {en_ans}</p>')
    lv9b.append('</div>')
    blocks.append(''.join(lv9b))

    # Lv.10
    s7 = all_data.get("step7", {})
    lv10 = ['<div class="ablock"><p class="ast">Stage 10 ì˜ì‘</p>']
    for idx, item in enumerate(s7.get("writing_items", []), start=1):
        lv10.append(f'<p>{idx}. {item.get("answer","")}</p>')
    lv10.append('</div>')
    blocks.append(''.join(lv10))

    answers_html = '\n'.join(blocks)
    data = {"answers_html": answers_html}
    save_step(passage_dir, "step8_answers", data)
    return data

# ============================================================
# ì „ì²´ ë°ì´í„° â†’ í…œí”Œë¦¿ ë³€ìˆ˜ë¡œ ë³€í™˜
# ============================================================
def merge_to_template_data(passage: str, meta: dict, all_steps: dict) -> dict:
    """ëª¨ë“  ë‹¨ê³„ ê²°ê³¼ë¥¼ í…œí”Œë¦¿ ë³€ìˆ˜ë¡œ ë³‘í•©"""
    s1 = all_steps["step1"]
    s2 = all_steps["step2"]
    s3 = all_steps["step3"]
    s4 = all_steps["step4"]
    s5 = all_steps["step5"]
    s6 = all_steps["step6"]
    s7 = all_steps["step7"]
    s8 = all_steps["step8"]

    return {
        # ë©”íƒ€ ì •ë³´
        "subject": meta.get("subject", ""),
        "publisher": meta.get("publisher", ""),
        "lesson_num": meta.get("lesson_num", ""),
        "lesson_n": meta.get("lesson_n", ""),
        "challenge_title": meta.get("challenge_title", ""),
        # ì§€ë¬¸/ë²ˆì—­
        "passage": passage,
        "translation": s1.get("translation", ""),
        # Lv.1 ì–´íœ˜
        "vocab": s1.get("vocab", []),
        "test_a": s1.get("test_a", []),
        "test_b": s1.get("test_b", []),
        "test_c": s1.get("test_c", []),
        # Lv.3 ë¬¸ì¥ë¶„ì„ (ì „ì²´ ë¬¸ì¥) + í•µì‹¬ë¬¸ì¥
        "sentences": s1.get("sentences", []),
        "key_sentences": s1.get("key_sentences", []),
        # Lv.5 ìˆœì„œ/ì‚½ì…
        "order_intro": s2.get("order_intro", ""),
        "order_paragraphs": s2.get("order_paragraphs", []),
        "order_choices": s2.get("order_choices", []),
        "insert_sentence": s2.get("insert_sentence", ""),
        "insert_passage": s2.get("insert_passage", ""),
        "full_order_blocks": s2.get("full_order_blocks", []),
        # Lv.6 ë¹ˆì¹¸
        "blank_passage": s3.get("blank_passage", ""),
        "blank_options": s3.get("blank_options", []),
        # Lv.7 ì£¼ì œ
        "topic_passage": s4.get("topic_passage", ""),
        "topic_options": s4.get("topic_options", []),
        # Lv.8 ì–´ë²•
        "grammar_bracket_passage": s5.get("grammar_bracket_passage", ""),
        "grammar_bracket_count": s5.get("grammar_bracket_count", 13),
        "grammar_error_passage": s5.get("grammar_error_passage", ""),
        "grammar_error_count": s5.get("grammar_error_count", 8),
        # Lv.9
        "vocab_advanced_passage": s6.get("vocab_advanced_passage", ""),
        "vocab_parta_answers": s6.get("vocab_parta_answers", []),
        "vocab_partb": s6.get("vocab_partb", []),
        "content_match_kr": s6.get("content_match_kr", []),
        "content_match_en": s6.get("content_match_en", []),
        # Stage 10 ì˜ì‘
        "writing_items": s7.get("writing_items", []),
        # ì •ë‹µ
        "answers_html": s8.get("answers_html", ""),
    }

# ============================================================
# PDF ë Œë”ë§
# ============================================================
def _unique_path(directory: Path, base_name: str, ext: str) -> Path:
    """ê°™ì€ ì´ë¦„ íŒŒì¼ì´ ìˆìœ¼ë©´ _v2, _v3 ë“± ë¶™ì—¬ì„œ ê³ ìœ  ê²½ë¡œ ë°˜í™˜"""
    path = directory / f"{base_name}{ext}"
    if not path.exists() and not path.with_suffix('.html').exists():
        return path
    v = 2
    while True:
        path = directory / f"{base_name}_v{v}{ext}"
        if not path.exists() and not path.with_suffix('.html').exists():
            return path
        v += 1

def render_pdf(template_data: dict, output_path: Path, levels=None):
    """Jinja2 â†’ HTML ì €ì¥ (í¬ë¡¬ì—ì„œ PDF ì¸ì‡„)"""
    from jinja2 import Environment, FileSystemLoader

    env = Environment(loader=FileSystemLoader(str(TEMPLATE_DIR)))
    tmpl = env.get_template("template.html")
    template_data["levels"] = levels  # Noneì´ë©´ ì „ì²´ ì¶œë ¥
    html = tmpl.render(**template_data)

    # WeasyPrint ì‹œë„, ì—†ìœ¼ë©´ HTMLë¡œ ì €ì¥
    try:
        from weasyprint import HTML
        HTML(string=html).write_pdf(str(output_path))
        _safe_print(f"  PDF created: {output_path.name}")
    except (ImportError, OSError):
        html_path = output_path.with_suffix('.html')
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html)
        _safe_print(f"  HTML created: {html_path.name}")
        _safe_print(f"  Open in Chrome -> Ctrl+P -> Save as PDF")

# ============================================================
# ë©”ì¸: ë‹¨ì¼ ì§€ë¬¸ ì²˜ë¦¬
# ============================================================
def process_passage(passage: str, meta: dict, passage_id: str, force=False, levels=None):
    """ì§€ë¬¸ 1ê°œ â†’ ì „ì²´ ì›Œí¬ë¶ ìƒì„±"""
    passage_dir = DATA_DIR / passage_id
    if force:
        import shutil
        if passage_dir.exists():
            shutil.rmtree(passage_dir)

    _safe_print(f"\n{'='*50}")
    _safe_print(f"Processing: {passage_id} ({meta.get('challenge_title','')})")
    _safe_print(f"{'='*50}")

    sentences = split_sentences(passage)
    all_steps = {}

    # Step 1: ê¸°ë³¸ ë¶„ì„
    all_steps["step1"] = step1_basic_analysis(passage, passage_dir)
    sentences_from_api = all_steps["step1"].get("sentences", sentences)

    # Step 2: Lv.5 ìˆœì„œ/ì‚½ì…
    all_steps["step2"] = step2_order(passage, sentences_from_api, passage_dir)

    # Step 3: Lv.6 ë¹ˆì¹¸
    all_steps["step3"] = step3_blank(passage, passage_dir)

    # Step 4: Lv.7 ì£¼ì œ
    all_steps["step4"] = step4_topic(passage, passage_dir)

    # Step 5: Lv.8 ì–´ë²•
    all_steps["step5"] = step5_grammar(passage, passage_dir)

    # Step 6: Lv.9 ì–´íœ˜+ë‚´ìš©ì¼ì¹˜
    all_steps["step6"] = step6_vocab_content(passage, passage_dir)

    # Step 7: Stage 10 ì˜ì‘ (ë¡œì»¬)
    translation = all_steps["step1"].get("translation", "")
    sentence_translations = all_steps["step1"].get("sentence_translations", [])
    all_steps["step7"] = step7_writing(sentences_from_api, translation, passage_dir, sentence_translations)

    # Step 8: ì •ë‹µ
    all_steps["step8"] = step8_answers(all_steps, passage_dir)

    # ë³‘í•© + PDF
    template_data = merge_to_template_data(passage, meta, all_steps)

    # ğŸ”’ ì½˜í…ì¸  ê¸¸ì´ ê²€ì¦ (í˜ì´ì§€ ë°€ë¦¼ ë°©ì§€)
    warnings = []
    bp = template_data.get("blank_passage", "")
    if len(bp) > 1200:
        warnings.append(f"blank_passage ê¸¸ì´ {len(bp)} (ê¶Œì¥ 1200 ì´ë‚´)")
    gp = template_data.get("grammar_bracket_passage", "")
    if len(gp) > 1600:
        warnings.append(f"grammar_bracket_passage ê¸¸ì´ {len(gp)} (ê¶Œì¥ 1600 ì´ë‚´)")
    gep = template_data.get("grammar_error_passage", "")
    if len(gep) > 1200:
        warnings.append(f"grammar_error_passage ê¸¸ì´ {len(gep)} (ê¶Œì¥ 1200 ì´ë‚´)")
    if warnings:
        _safe_print(f"  WARNING: content length warning:")
        for w in warnings:
            _safe_print(f"     - {w}")

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    base_name = f"{meta.get('lesson_num','')}ê³¼_{meta.get('challenge_title','ì›Œí¬ë¶')}_ì›Œí¬ë¶"
    pdf_path = _unique_path(OUTPUT_DIR, base_name, ".pdf")
    render_pdf(template_data, pdf_path, levels=levels)

    _safe_print(f"Done: {pdf_path.name}")
    return pdf_path

# ============================================================
# ë°°ì¹˜ ì²˜ë¦¬: ì—¬ëŸ¬ ì§€ë¬¸
# ============================================================
def process_batch(passages: list[dict], levels=None):
    """ì—¬ëŸ¬ ì§€ë¬¸ì„ ìˆœì°¨ ì²˜ë¦¬
    passages: [{"passage": "...", "meta": {...}, "id": "01"}, ...]
    """
    results = []
    total = len(passages)
    for i, item in enumerate(passages):
        _safe_print(f"\n[{i+1}/{total}] Processing...")
        try:
            pdf = process_passage(item["passage"], item["meta"], item["id"], levels=levels)
            results.append({"id": item["id"], "status": "done", "pdf": str(pdf)})
        except Exception as e:
            _safe_print(f"FAILED: {item['id']} - {e}")
            results.append({"id": item["id"], "status": "error", "error": str(e)})

    # ê²°ê³¼ ìš”ì•½
    _safe_print(f"\n{'='*50}")
    _safe_print(" Results summary")
    done = sum(1 for r in results if r["status"] == "done")
    err = sum(1 for r in results if r["status"] == "error")
    _safe_print(f"  Success: {done}/{total}")
    if err:
        _safe_print(f"  Failed: {err}/{total}")
        for r in results:
            if r["status"] == "error":
                _safe_print(f"     - {r['id']}: {r['error']}")
    return results


# ============================================================
# ë‹¨ì¼ íŒŒì¼ì—ì„œ ì—¬ëŸ¬ ì§€ë¬¸ ìë™ ë¶„ë¦¬ + ì‹¤í–‰
# ============================================================
def split_and_run(filepath: str, lesson_num: str = "5", levels=None):
    """
    ###ì œëª©### êµ¬ë¶„ìë¡œ ë‚˜ë‰œ ë‹¨ì¼ íŒŒì¼ì—ì„œ ì§€ë¬¸ ì¶”ì¶œ â†’ ìˆœì°¨ ì‹¤í–‰
    
    íŒŒì¼ í˜•ì‹:
        ###05ê°• 01ë²ˆ###
        ì§€ë¬¸ ì˜ì–´ í…ìŠ¤íŠ¸...
        
        ###05ê°• 02ë²ˆ###
        ì§€ë¬¸ ì˜ì–´ í…ìŠ¤íŠ¸...
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ###...### íŒ¨í„´ìœ¼ë¡œ ë¶„ë¦¬
    parts = re.split(r'###(.+?)###', content)
    # parts = ['', '05ê°• 01ë²ˆ', 'ì§€ë¬¸ë‚´ìš©', '05ê°• 02ë²ˆ', 'ì§€ë¬¸ë‚´ìš©', ...]
    
    passages = []
    for i in range(1, len(parts), 2):
        title = parts[i].strip()
        text = parts[i+1].strip() if i+1 < len(parts) else ""
        if text:
            # ì œëª©ì—ì„œ ê³¼ë²ˆí˜¸ + ë²ˆí˜¸ ì¶”ì¶œ
            lesson_match = re.search(r'(\d+)ê°•', title)
            num_match = re.search(r'(\d+)ë²ˆ', title)
            lnum = lesson_match.group(1) if lesson_match else lesson_num
            pid = num_match.group(1).zfill(2) if num_match else str(len(passages)+1).zfill(2)
            passages.append({
                "id": f"{lnum}_{pid}",
                "passage": text,
                "meta": {
                    "subject": "ìˆ˜íŠ¹ ì˜ì–´", "publisher": "EBS",
                    "lesson_num": lnum, "lesson_n": lnum,
                    "challenge_title": title
                }
            })
    
    if not passages:
        _safe_print("ERROR: No passages found. Check ### format.")
        return
    
    _safe_print(f"Found {len(passages)} passages")
    for p in passages:
        _safe_print(f"  - {p['meta']['challenge_title']}")
    print()
    
    process_batch(passages, levels=levels)

    # ìë™ìœ¼ë¡œ HTML í•©ì¹˜ê¸°
    merge_html_files()


# ============================================================
# HTML í•©ì¹˜ê¸° (ì—¬ëŸ¬ ì›Œí¬ë¶ â†’ í•˜ë‚˜ì˜ HTML)
# ============================================================
def merge_html_files(output_dir=None):
    """output í´ë”ì˜ ëª¨ë“  HTMLì„ í•˜ë‚˜ë¡œ í•©ì¹¨ (í•©ë³¸ íŒŒì¼ëª… ìë™ ìƒì„±)"""
    if output_dir is None:
        output_dir = OUTPUT_DIR
    
    html_files = sorted([f for f in output_dir.glob("*ì›Œí¬ë¶.html") if "í•©ë³¸" not in f.name])
    if len(html_files) < 2:
        return
    
    _safe_print(f"\nMerging {len(html_files)} HTML files...")
    
    # íŒŒì¼ëª…ì—ì„œ ì œëª© ì¶”ì¶œí•˜ì—¬ í•©ë³¸ëª… ìƒì„±
    import re as _re
    titles = []
    for hf in html_files:
        # "3ê³¼_03ê°•_02ë²ˆ_ì›Œí¬ë¶.html" â†’ "03ê°• 02ë²ˆ"
        m = _re.search(r'(\d+ê°•)[_ ](\w+)', hf.stem.replace('_ì›Œí¬ë¶',''))
        if m:
            titles.append(f"{m.group(1)} {m.group(2)}")
    
    if titles:
        first = titles[0]   # ì˜ˆ: "03ê°• 01ë²ˆ"
        last = titles[-1]   # ì˜ˆ: "04ê°• 04ë²ˆ"
        merge_name = f"{first}~{last} í•©ë³¸.html"
    else:
        merge_name = "ì „ì²´_ì›Œí¬ë¶_í•©ë³¸.html"
    
    # ì²« íŒŒì¼ì—ì„œ CSS ì¶”ì¶œ
    first_html = html_files[0].read_text(encoding='utf-8')
    style_match = _re.search(r'<style[^>]*>(.*?)</style>', first_html, _re.DOTALL)
    css = style_match.group(1) if style_match else ""
    
    # ê° íŒŒì¼ì—ì„œ <body> ë‚´ìš©ë§Œ ì¶”ì¶œ
    all_bodies = []
    for hf in html_files:
        html = hf.read_text(encoding='utf-8')
        body_match = _re.search(r'<body[^>]*>(.*?)</body>', html, _re.DOTALL)
        if body_match:
            all_bodies.append(body_match.group(1))
    
    # í•©ì¹œ HTML ìƒì„±
    merged_path = _unique_path(output_dir, merge_name.replace('.html', ''), '.html')
    merged = f"""<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="UTF-8">
<title>{merged_path.stem}</title>
<style>
{css}
</style>
</head>
<body>
{''.join(all_bodies)}
</body>
</html>"""
    
    merged_path.write_text(merged, encoding='utf-8')
    _safe_print(f"  Merged: {merged_path.name}")
    _safe_print(f"  Open in Chrome -> Ctrl+P -> Save as PDF")
if __name__ == "__main__":
    # --level íŒŒì‹± (ì–´ë””ì„œë“  ì‚¬ìš© ê°€ëŠ¥)
    levels = None
    filtered_args = []
    for i, arg in enumerate(sys.argv):
        if arg == "--level" and i+1 < len(sys.argv):
            levels = [int(x) for x in sys.argv[i+1].split(",")]
            continue
        if i > 0 and sys.argv[i-1] == "--level":
            continue
        filtered_args.append(arg)
    sys.argv = filtered_args

    if len(sys.argv) < 2:
        print("Usage:")
        _safe_print("  Multiple: py pipeline.py --all all.txt")
        _safe_print("  Levels: py pipeline.py --all all.txt --level 1,2,5,8")
        _safe_print("  Single: py pipeline.py passage.txt 5 \"05-01\"")
        _safe_print("  Merge: py pipeline.py --merge")
        print()
        _safe_print("  --level option: select levels (0=cover+answers)")
        _safe_print("    e.g.) --level 1,2,3,4")
        _safe_print("    e.g.) --level 5,6,7,8")
        _safe_print("    e.g.) --level 0,1,2")
        sys.exit(1)

    if levels:
        _safe_print(f"Level filter: Lv.{','.join(str(l) for l in levels)}")

    if sys.argv[1] == "--merge":
        merge_html_files()
    elif sys.argv[1] == "--all":
        filepath = sys.argv[2]
        lesson = sys.argv[3] if len(sys.argv) > 3 else "5"
        split_and_run(filepath, lesson, levels=levels)
    elif sys.argv[1] == "--batch":
        with open(sys.argv[2], 'r', encoding='utf-8') as f:
            batch = json.load(f)
        process_batch(batch, levels=levels)
    else:
        passage_file = sys.argv[1]
        with open(passage_file, 'r', encoding='utf-8') as f:
            passage = f.read().strip()
        lesson_num = sys.argv[2] if len(sys.argv) > 2 else "1"
        title = sys.argv[3] if len(sys.argv) > 3 else Path(passage_file).stem
        meta = {
            "subject": "ìˆ˜íŠ¹ ì˜ì–´", "publisher": "EBS",
            "lesson_num": lesson_num, "lesson_n": lesson_num,
            "challenge_title": title
        }
        process_passage(passage, meta, f"passage_{lesson_num}_{title}", levels=levels)
